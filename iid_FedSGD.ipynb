{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "n_train_items = 6000\n",
    "rounds = 650\n",
    "total_client = 100\n",
    "C = 0.1\n",
    "n_workers = int(total_client * C)\n",
    "batch_size = 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "\n",
    "workers = connect_to_workers(n_workers=n_workers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = batch_size\n",
    "        self.test_batch_size = 60\n",
    "        self.epochs = epochs\n",
    "        self.rounds = rounds\n",
    "        self.lr = 0.001\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 2\n",
    "        self.save_model = False\n",
    "        self.n_train_items = n_train_items\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ]))\n",
    "#     .federate(workers), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "# less_train_dataloader = [\n",
    "#         ((data), (target))\n",
    "#         for i, (data, target) in enumerate(train_loader)\n",
    "#         if i < (n_train_items / args.batch_size) * 10\n",
    "#     ]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n"
     ]
    }
   ],
   "source": [
    "print(len(train_loader))\n",
    "# print(len(less_train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy \n",
    "# #mnist_dataset.__getitem__(2)[1]\n",
    "# a = (mnist_dataset.__getitem__(0)[0]).numpy()\n",
    "# a.dtype = 'uint8'\n",
    "# print(a)\n",
    "# Image.fromarray(a[0], mode= 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*64, 512)\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*64)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(workers, Net):\n",
    "    model_list = list()\n",
    "    for worker in workers:\n",
    "        model_list.append(Net)\n",
    "    return model_list\n",
    "def opt_init(model_list):\n",
    "    opt_list = list()\n",
    "    for model  in model_list:\n",
    "        opt_list.append(optim.SGD(model.parameters(), lr=args.lr))\n",
    "    return opt_list\n",
    "def random_sample(train_dataloader):\n",
    "    choice_list = sorted(random.sample(range(100), 10))\n",
    "    count = 0\n",
    "    tmp = []\n",
    "    for i, data in enumerate(train_dataloader):\n",
    "        if  i == choice_list[count]:\n",
    "            tmp.append(data)\n",
    "            if count == 9:\n",
    "                pass\n",
    "            else:\n",
    "                count += 1\n",
    "    return tmp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device, train_loader, opt_list, workers):\n",
    "    global model_list\n",
    "    ## start training and record the model into model_list\n",
    "    \n",
    "    less_train_dataloader = random_sample(train_loader)\n",
    "    for epoch in range(args.epochs):\n",
    "        for batch_idx, (data, target) in enumerate(less_train_dataloader): # <-- now it is a distributed dataset\n",
    "            model_on_worker = model_list[batch_idx%len(workers)]\n",
    "            model_on_worker.train()\n",
    "            model_on_worker.send(workers[batch_idx%len(workers)]) # <-- NEW: send the model to the right location\n",
    "\n",
    "            data_on_worker = data.send(workers[batch_idx%len(workers)])\n",
    "            target_on_worker = target.send(workers[batch_idx%len(workers)])\n",
    "\n",
    "            data_on_worker, target_on_worker = data_on_worker.to(device), target_on_worker.to(device)\n",
    "\n",
    "            opt_list[batch_idx%len(workers)].zero_grad()\n",
    "\n",
    "            output = model_on_worker(data_on_worker)\n",
    "            loss = F.nll_loss(output, target_on_worker)\n",
    "            loss.backward()\n",
    "\n",
    "            opt_list[batch_idx%len(workers)].step()\n",
    "            model_on_worker.get() # <-- NEW: get the model back\n",
    "\n",
    "            model_list[batch_idx%len(workers)] = model_on_worker #When len(dataloader) is longer than the len(worker) send and get must be modified\n",
    "            #model_list here is full of the model which has trained on the workers, there are all different now.\n",
    "\n",
    "        if epoch % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, n_train_items, n_train_items ,\n",
    "                100. * epoch / args.epochs, loss.item()))\n",
    "\n",
    "\n",
    "    ##Aggregation time\n",
    "    new_model = []\n",
    "    tmp_model = Net().to(device)\n",
    "    with torch.no_grad():\n",
    "        for p in model_list[0].parameters():\n",
    "            new_model.append(0)\n",
    "            \n",
    "        for m in model_list:\n",
    "            for par_idx, par in enumerate(m.parameters()):\n",
    "                #average the model_list\n",
    "                new_model[par_idx] = new_model[par_idx]+par.data\n",
    "                # we get new model in list format and need to set_ to model\n",
    "        for worker in range(len(workers)):\n",
    "            for par_idx in range(len(new_model)):\n",
    "                list(model_list[worker].parameters())[par_idx].set_(new_model[par_idx]/len(workers))\n",
    "        #init model with new_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader, r):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader)*(args.test_batch_size)\n",
    "    accuracy = 100. * correct / (len(test_loader)*args.test_batch_size)\n",
    "    #Since the test loader here is a list, we can get the len by * it with batch.size\n",
    "    \n",
    "    \n",
    "    writer.add_scalar('Accuracy', accuracy,r)\n",
    "    writer.add_scalar('Loss', test_loss, r)\n",
    "    print('\\nTest set in round{}: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        r , test_loss, correct, len(test_loader)* (args.test_batch_size),\n",
    "        accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.292366\n",
      "After training\n",
      "\n",
      "Test set in round1: Average loss: 2.2867, Accuracy: 1607/10020 (16.04%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.288422\n",
      "After training\n",
      "\n",
      "Test set in round2: Average loss: 2.2809, Accuracy: 1773/10020 (17.69%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.278244\n",
      "After training\n",
      "\n",
      "Test set in round3: Average loss: 2.2754, Accuracy: 1961/10020 (19.57%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.276421\n",
      "After training\n",
      "\n",
      "Test set in round4: Average loss: 2.2697, Accuracy: 2159/10020 (21.55%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.272597\n",
      "After training\n",
      "\n",
      "Test set in round5: Average loss: 2.2639, Accuracy: 2322/10020 (23.17%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.265527\n",
      "After training\n",
      "\n",
      "Test set in round6: Average loss: 2.2582, Accuracy: 2492/10020 (24.87%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.268945\n",
      "After training\n",
      "\n",
      "Test set in round7: Average loss: 2.2526, Accuracy: 2636/10020 (26.31%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.254922\n",
      "After training\n",
      "\n",
      "Test set in round8: Average loss: 2.2471, Accuracy: 2748/10020 (27.43%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.254886\n",
      "After training\n",
      "\n",
      "Test set in round9: Average loss: 2.2415, Accuracy: 2907/10020 (29.01%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.240137\n",
      "After training\n",
      "\n",
      "Test set in round10: Average loss: 2.2356, Accuracy: 3082/10020 (30.76%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.238353\n",
      "After training\n",
      "\n",
      "Test set in round11: Average loss: 2.2298, Accuracy: 3198/10020 (31.92%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.223048\n",
      "After training\n",
      "\n",
      "Test set in round12: Average loss: 2.2238, Accuracy: 3373/10020 (33.66%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.225239\n",
      "After training\n",
      "\n",
      "Test set in round13: Average loss: 2.2177, Accuracy: 3532/10020 (35.25%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.216268\n",
      "After training\n",
      "\n",
      "Test set in round14: Average loss: 2.2115, Accuracy: 3691/10020 (36.84%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.211564\n",
      "After training\n",
      "\n",
      "Test set in round15: Average loss: 2.2053, Accuracy: 3829/10020 (38.21%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.203183\n",
      "After training\n",
      "\n",
      "Test set in round16: Average loss: 2.1989, Accuracy: 4045/10020 (40.37%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.208068\n",
      "After training\n",
      "\n",
      "Test set in round17: Average loss: 2.1925, Accuracy: 4205/10020 (41.97%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.188398\n",
      "After training\n",
      "\n",
      "Test set in round18: Average loss: 2.1860, Accuracy: 4321/10020 (43.12%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.181864\n",
      "After training\n",
      "\n",
      "Test set in round19: Average loss: 2.1793, Accuracy: 4519/10020 (45.10%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.174756\n",
      "After training\n",
      "\n",
      "Test set in round20: Average loss: 2.1725, Accuracy: 4667/10020 (46.58%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.178401\n",
      "After training\n",
      "\n",
      "Test set in round21: Average loss: 2.1654, Accuracy: 4796/10020 (47.86%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.162641\n",
      "After training\n",
      "\n",
      "Test set in round22: Average loss: 2.1582, Accuracy: 4980/10020 (49.70%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.155128\n",
      "After training\n",
      "\n",
      "Test set in round23: Average loss: 2.1508, Accuracy: 5170/10020 (51.60%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.156961\n",
      "After training\n",
      "\n",
      "Test set in round24: Average loss: 2.1433, Accuracy: 5330/10020 (53.19%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.139877\n",
      "After training\n",
      "\n",
      "Test set in round25: Average loss: 2.1356, Accuracy: 5610/10020 (55.99%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.129809\n",
      "After training\n",
      "\n",
      "Test set in round26: Average loss: 2.1275, Accuracy: 5717/10020 (57.06%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.121415\n",
      "After training\n",
      "\n",
      "Test set in round27: Average loss: 2.1192, Accuracy: 5809/10020 (57.97%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.122683\n",
      "After training\n",
      "\n",
      "Test set in round28: Average loss: 2.1109, Accuracy: 5907/10020 (58.95%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.118944\n",
      "After training\n",
      "\n",
      "Test set in round29: Average loss: 2.1023, Accuracy: 5973/10020 (59.61%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.099036\n",
      "After training\n",
      "\n",
      "Test set in round30: Average loss: 2.0934, Accuracy: 6210/10020 (61.98%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.072636\n",
      "After training\n",
      "\n",
      "Test set in round31: Average loss: 2.0840, Accuracy: 6304/10020 (62.91%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.090931\n",
      "After training\n",
      "\n",
      "Test set in round32: Average loss: 2.0745, Accuracy: 6467/10020 (64.54%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.061745\n",
      "After training\n",
      "\n",
      "Test set in round33: Average loss: 2.0647, Accuracy: 6564/10020 (65.51%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.066333\n",
      "After training\n",
      "\n",
      "Test set in round34: Average loss: 2.0545, Accuracy: 6627/10020 (66.14%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.065314\n",
      "After training\n",
      "\n",
      "Test set in round35: Average loss: 2.0441, Accuracy: 6659/10020 (66.46%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.029680\n",
      "After training\n",
      "\n",
      "Test set in round36: Average loss: 2.0332, Accuracy: 6709/10020 (66.96%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.038990\n",
      "After training\n",
      "\n",
      "Test set in round37: Average loss: 2.0220, Accuracy: 6837/10020 (68.23%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.014065\n",
      "After training\n",
      "\n",
      "Test set in round38: Average loss: 2.0105, Accuracy: 6963/10020 (69.49%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.012364\n",
      "After training\n",
      "\n",
      "Test set in round39: Average loss: 1.9989, Accuracy: 7030/10020 (70.16%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.016753\n",
      "After training\n",
      "\n",
      "Test set in round40: Average loss: 1.9869, Accuracy: 7061/10020 (70.47%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 2.000086\n",
      "After training\n",
      "\n",
      "Test set in round41: Average loss: 1.9744, Accuracy: 7100/10020 (70.86%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.941001\n",
      "After training\n",
      "\n",
      "Test set in round42: Average loss: 1.9613, Accuracy: 7119/10020 (71.05%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.946628\n",
      "After training\n",
      "\n",
      "Test set in round43: Average loss: 1.9479, Accuracy: 7156/10020 (71.42%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.929649\n",
      "After training\n",
      "\n",
      "Test set in round44: Average loss: 1.9339, Accuracy: 7216/10020 (72.02%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.934573\n",
      "After training\n",
      "\n",
      "Test set in round45: Average loss: 1.9198, Accuracy: 7266/10020 (72.51%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.909014\n",
      "After training\n",
      "\n",
      "Test set in round46: Average loss: 1.9048, Accuracy: 7323/10020 (73.08%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.890908\n",
      "After training\n",
      "\n",
      "Test set in round47: Average loss: 1.8895, Accuracy: 7374/10020 (73.59%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.892177\n",
      "After training\n",
      "\n",
      "Test set in round48: Average loss: 1.8738, Accuracy: 7414/10020 (73.99%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.836031\n",
      "After training\n",
      "\n",
      "Test set in round49: Average loss: 1.8574, Accuracy: 7418/10020 (74.03%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.810769\n",
      "After training\n",
      "\n",
      "Test set in round50: Average loss: 1.8405, Accuracy: 7452/10020 (74.37%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.819009\n",
      "After training\n",
      "\n",
      "Test set in round51: Average loss: 1.8229, Accuracy: 7479/10020 (74.64%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.821380\n",
      "After training\n",
      "\n",
      "Test set in round52: Average loss: 1.8058, Accuracy: 7492/10020 (74.77%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.786883\n",
      "After training\n",
      "\n",
      "Test set in round53: Average loss: 1.7875, Accuracy: 7486/10020 (74.71%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.788763\n",
      "After training\n",
      "\n",
      "Test set in round54: Average loss: 1.7686, Accuracy: 7512/10020 (74.97%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.739690\n",
      "After training\n",
      "\n",
      "Test set in round55: Average loss: 1.7495, Accuracy: 7541/10020 (75.26%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.739308\n",
      "After training\n",
      "\n",
      "Test set in round56: Average loss: 1.7302, Accuracy: 7580/10020 (75.65%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.708926\n",
      "After training\n",
      "\n",
      "Test set in round57: Average loss: 1.7102, Accuracy: 7590/10020 (75.75%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.681933\n",
      "After training\n",
      "\n",
      "Test set in round58: Average loss: 1.6895, Accuracy: 7618/10020 (76.03%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.661019\n",
      "After training\n",
      "\n",
      "Test set in round59: Average loss: 1.6684, Accuracy: 7642/10020 (76.27%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.627503\n",
      "After training\n",
      "\n",
      "Test set in round60: Average loss: 1.6468, Accuracy: 7649/10020 (76.34%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.617185\n",
      "After training\n",
      "\n",
      "Test set in round61: Average loss: 1.6251, Accuracy: 7677/10020 (76.62%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.626765\n",
      "After training\n",
      "\n",
      "Test set in round62: Average loss: 1.6029, Accuracy: 7711/10020 (76.96%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.530626\n",
      "After training\n",
      "\n",
      "Test set in round63: Average loss: 1.5799, Accuracy: 7751/10020 (77.36%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.539188\n",
      "After training\n",
      "\n",
      "Test set in round64: Average loss: 1.5567, Accuracy: 7761/10020 (77.46%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.530542\n",
      "After training\n",
      "\n",
      "Test set in round65: Average loss: 1.5337, Accuracy: 7770/10020 (77.54%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.512947\n",
      "After training\n",
      "\n",
      "Test set in round66: Average loss: 1.5103, Accuracy: 7773/10020 (77.57%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.463108\n",
      "After training\n",
      "\n",
      "Test set in round67: Average loss: 1.4865, Accuracy: 7792/10020 (77.76%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.437202\n",
      "After training\n",
      "\n",
      "Test set in round68: Average loss: 1.4624, Accuracy: 7847/10020 (78.31%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.435150\n",
      "After training\n",
      "\n",
      "Test set in round69: Average loss: 1.4379, Accuracy: 7848/10020 (78.32%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.420804\n",
      "After training\n",
      "\n",
      "Test set in round70: Average loss: 1.4132, Accuracy: 7903/10020 (78.87%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.406323\n",
      "After training\n",
      "\n",
      "Test set in round71: Average loss: 1.3892, Accuracy: 7969/10020 (79.53%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.373415\n",
      "After training\n",
      "\n",
      "Test set in round72: Average loss: 1.3651, Accuracy: 7961/10020 (79.45%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.331698\n",
      "After training\n",
      "\n",
      "Test set in round73: Average loss: 1.3402, Accuracy: 7965/10020 (79.49%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.296230\n",
      "After training\n",
      "\n",
      "Test set in round74: Average loss: 1.3163, Accuracy: 7960/10020 (79.44%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.365083\n",
      "After training\n",
      "\n",
      "Test set in round75: Average loss: 1.2921, Accuracy: 8015/10020 (79.99%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.235140\n",
      "After training\n",
      "\n",
      "Test set in round76: Average loss: 1.2680, Accuracy: 8032/10020 (80.16%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.312289\n",
      "After training\n",
      "\n",
      "Test set in round77: Average loss: 1.2446, Accuracy: 8048/10020 (80.32%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.216083\n",
      "After training\n",
      "\n",
      "Test set in round78: Average loss: 1.2213, Accuracy: 8064/10020 (80.48%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.171735\n",
      "After training\n",
      "\n",
      "Test set in round79: Average loss: 1.1972, Accuracy: 8096/10020 (80.80%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.165071\n",
      "After training\n",
      "\n",
      "Test set in round80: Average loss: 1.1749, Accuracy: 8085/10020 (80.69%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.071397\n",
      "After training\n",
      "\n",
      "Test set in round81: Average loss: 1.1517, Accuracy: 8153/10020 (81.37%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.128228\n",
      "After training\n",
      "\n",
      "Test set in round82: Average loss: 1.1291, Accuracy: 8193/10020 (81.77%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.174621\n",
      "After training\n",
      "\n",
      "Test set in round83: Average loss: 1.1071, Accuracy: 8204/10020 (81.88%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.095308\n",
      "After training\n",
      "\n",
      "Test set in round84: Average loss: 1.0861, Accuracy: 8210/10020 (81.94%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.073406\n",
      "After training\n",
      "\n",
      "Test set in round85: Average loss: 1.0643, Accuracy: 8263/10020 (82.47%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.007331\n",
      "After training\n",
      "\n",
      "Test set in round86: Average loss: 1.0438, Accuracy: 8269/10020 (82.52%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 1.033443\n",
      "After training\n",
      "\n",
      "Test set in round87: Average loss: 1.0237, Accuracy: 8266/10020 (82.50%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.955469\n",
      "After training\n",
      "\n",
      "Test set in round88: Average loss: 1.0036, Accuracy: 8272/10020 (82.55%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.941946\n",
      "After training\n",
      "\n",
      "Test set in round89: Average loss: 0.9846, Accuracy: 8283/10020 (82.66%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.873667\n",
      "After training\n",
      "\n",
      "Test set in round90: Average loss: 0.9652, Accuracy: 8325/10020 (83.08%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.939766\n",
      "After training\n",
      "\n",
      "Test set in round91: Average loss: 0.9465, Accuracy: 8344/10020 (83.27%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.929977\n",
      "After training\n",
      "\n",
      "Test set in round92: Average loss: 0.9287, Accuracy: 8340/10020 (83.23%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.951145\n",
      "After training\n",
      "\n",
      "Test set in round93: Average loss: 0.9112, Accuracy: 8383/10020 (83.66%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.940770\n",
      "After training\n",
      "\n",
      "Test set in round94: Average loss: 0.8942, Accuracy: 8402/10020 (83.85%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.826933\n",
      "After training\n",
      "\n",
      "Test set in round95: Average loss: 0.8784, Accuracy: 8396/10020 (83.79%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.855951\n",
      "After training\n",
      "\n",
      "Test set in round96: Average loss: 0.8623, Accuracy: 8394/10020 (83.77%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.746758\n",
      "After training\n",
      "\n",
      "Test set in round97: Average loss: 0.8463, Accuracy: 8445/10020 (84.28%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.729489\n",
      "After training\n",
      "\n",
      "Test set in round98: Average loss: 0.8314, Accuracy: 8461/10020 (84.44%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.886807\n",
      "After training\n",
      "\n",
      "Test set in round99: Average loss: 0.8171, Accuracy: 8457/10020 (84.40%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.815079\n",
      "After training\n",
      "\n",
      "Test set in round100: Average loss: 0.8038, Accuracy: 8469/10020 (84.52%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.807364\n",
      "After training\n",
      "\n",
      "Test set in round101: Average loss: 0.7907, Accuracy: 8475/10020 (84.58%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.716401\n",
      "After training\n",
      "\n",
      "Test set in round102: Average loss: 0.7789, Accuracy: 8449/10020 (84.32%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.831411\n",
      "After training\n",
      "\n",
      "Test set in round103: Average loss: 0.7651, Accuracy: 8490/10020 (84.73%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.689178\n",
      "After training\n",
      "\n",
      "Test set in round104: Average loss: 0.7532, Accuracy: 8498/10020 (84.81%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.676286\n",
      "After training\n",
      "\n",
      "Test set in round105: Average loss: 0.7411, Accuracy: 8501/10020 (84.84%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.688313\n",
      "After training\n",
      "\n",
      "Test set in round106: Average loss: 0.7292, Accuracy: 8550/10020 (85.33%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.652044\n",
      "After training\n",
      "\n",
      "Test set in round107: Average loss: 0.7183, Accuracy: 8536/10020 (85.19%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.690171\n",
      "After training\n",
      "\n",
      "Test set in round108: Average loss: 0.7081, Accuracy: 8557/10020 (85.40%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.631088\n",
      "After training\n",
      "\n",
      "Test set in round109: Average loss: 0.6982, Accuracy: 8540/10020 (85.23%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.755024\n",
      "After training\n",
      "\n",
      "Test set in round110: Average loss: 0.6877, Accuracy: 8545/10020 (85.28%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.695981\n",
      "After training\n",
      "\n",
      "Test set in round111: Average loss: 0.6782, Accuracy: 8567/10020 (85.50%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.601624\n",
      "After training\n",
      "\n",
      "Test set in round112: Average loss: 0.6688, Accuracy: 8578/10020 (85.61%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.651382\n",
      "After training\n",
      "\n",
      "Test set in round113: Average loss: 0.6600, Accuracy: 8581/10020 (85.64%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.605329\n",
      "After training\n",
      "\n",
      "Test set in round114: Average loss: 0.6512, Accuracy: 8596/10020 (85.79%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.630497\n",
      "After training\n",
      "\n",
      "Test set in round115: Average loss: 0.6429, Accuracy: 8596/10020 (85.79%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.683498\n",
      "After training\n",
      "\n",
      "Test set in round116: Average loss: 0.6350, Accuracy: 8615/10020 (85.98%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.586319\n",
      "After training\n",
      "\n",
      "Test set in round117: Average loss: 0.6268, Accuracy: 8629/10020 (86.12%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.557952\n",
      "After training\n",
      "\n",
      "Test set in round118: Average loss: 0.6191, Accuracy: 8644/10020 (86.27%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.499949\n",
      "After training\n",
      "\n",
      "Test set in round119: Average loss: 0.6112, Accuracy: 8653/10020 (86.36%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.592396\n",
      "After training\n",
      "\n",
      "Test set in round120: Average loss: 0.6047, Accuracy: 8645/10020 (86.28%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.584846\n",
      "After training\n",
      "\n",
      "Test set in round121: Average loss: 0.5981, Accuracy: 8656/10020 (86.39%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.478009\n",
      "After training\n",
      "\n",
      "Test set in round122: Average loss: 0.5905, Accuracy: 8671/10020 (86.54%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.470957\n",
      "After training\n",
      "\n",
      "Test set in round123: Average loss: 0.5844, Accuracy: 8672/10020 (86.55%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.508383\n",
      "After training\n",
      "\n",
      "Test set in round124: Average loss: 0.5783, Accuracy: 8672/10020 (86.55%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.619416\n",
      "After training\n",
      "\n",
      "Test set in round125: Average loss: 0.5721, Accuracy: 8679/10020 (86.62%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.545411\n",
      "After training\n",
      "\n",
      "Test set in round126: Average loss: 0.5663, Accuracy: 8691/10020 (86.74%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.576699\n",
      "After training\n",
      "\n",
      "Test set in round127: Average loss: 0.5599, Accuracy: 8695/10020 (86.78%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.634533\n",
      "After training\n",
      "\n",
      "Test set in round128: Average loss: 0.5540, Accuracy: 8709/10020 (86.92%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.510322\n",
      "After training\n",
      "\n",
      "Test set in round129: Average loss: 0.5485, Accuracy: 8724/10020 (87.07%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.586549\n",
      "After training\n",
      "\n",
      "Test set in round130: Average loss: 0.5438, Accuracy: 8712/10020 (86.95%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.447212\n",
      "After training\n",
      "\n",
      "Test set in round131: Average loss: 0.5380, Accuracy: 8730/10020 (87.13%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.465052\n",
      "After training\n",
      "\n",
      "Test set in round132: Average loss: 0.5339, Accuracy: 8737/10020 (87.20%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.576671\n",
      "After training\n",
      "\n",
      "Test set in round133: Average loss: 0.5282, Accuracy: 8730/10020 (87.13%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.584261\n",
      "After training\n",
      "\n",
      "Test set in round134: Average loss: 0.5234, Accuracy: 8739/10020 (87.22%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.499778\n",
      "After training\n",
      "\n",
      "Test set in round135: Average loss: 0.5195, Accuracy: 8734/10020 (87.17%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.487489\n",
      "After training\n",
      "\n",
      "Test set in round136: Average loss: 0.5145, Accuracy: 8745/10020 (87.28%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.559912\n",
      "After training\n",
      "\n",
      "Test set in round137: Average loss: 0.5104, Accuracy: 8763/10020 (87.46%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.482703\n",
      "After training\n",
      "\n",
      "Test set in round138: Average loss: 0.5073, Accuracy: 8751/10020 (87.34%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.434759\n",
      "After training\n",
      "\n",
      "Test set in round139: Average loss: 0.5024, Accuracy: 8776/10020 (87.58%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.548753\n",
      "After training\n",
      "\n",
      "Test set in round140: Average loss: 0.4971, Accuracy: 8790/10020 (87.72%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.394047\n",
      "After training\n",
      "\n",
      "Test set in round141: Average loss: 0.4934, Accuracy: 8773/10020 (87.55%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.464559\n",
      "After training\n",
      "\n",
      "Test set in round142: Average loss: 0.4895, Accuracy: 8797/10020 (87.79%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.367213\n",
      "After training\n",
      "\n",
      "Test set in round143: Average loss: 0.4857, Accuracy: 8783/10020 (87.65%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.383622\n",
      "After training\n",
      "\n",
      "Test set in round144: Average loss: 0.4819, Accuracy: 8802/10020 (87.84%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.377102\n",
      "After training\n",
      "\n",
      "Test set in round145: Average loss: 0.4787, Accuracy: 8801/10020 (87.83%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.481644\n",
      "After training\n",
      "\n",
      "Test set in round146: Average loss: 0.4741, Accuracy: 8811/10020 (87.93%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.442846\n",
      "After training\n",
      "\n",
      "Test set in round147: Average loss: 0.4727, Accuracy: 8825/10020 (88.07%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.517438\n",
      "After training\n",
      "\n",
      "Test set in round148: Average loss: 0.4676, Accuracy: 8815/10020 (87.97%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.399148\n",
      "After training\n",
      "\n",
      "Test set in round149: Average loss: 0.4646, Accuracy: 8825/10020 (88.07%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.437218\n",
      "After training\n",
      "\n",
      "Test set in round150: Average loss: 0.4622, Accuracy: 8830/10020 (88.12%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.393369\n",
      "After training\n",
      "\n",
      "Test set in round151: Average loss: 0.4593, Accuracy: 8831/10020 (88.13%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.461550\n",
      "After training\n",
      "\n",
      "Test set in round152: Average loss: 0.4557, Accuracy: 8855/10020 (88.37%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.334869\n",
      "After training\n",
      "\n",
      "Test set in round153: Average loss: 0.4526, Accuracy: 8849/10020 (88.31%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.385258\n",
      "After training\n",
      "\n",
      "Test set in round154: Average loss: 0.4499, Accuracy: 8851/10020 (88.33%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.483061\n",
      "After training\n",
      "\n",
      "Test set in round155: Average loss: 0.4468, Accuracy: 8858/10020 (88.40%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.477759\n",
      "After training\n",
      "\n",
      "Test set in round156: Average loss: 0.4435, Accuracy: 8868/10020 (88.50%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.464602\n",
      "After training\n",
      "\n",
      "Test set in round157: Average loss: 0.4411, Accuracy: 8858/10020 (88.40%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.486562\n",
      "After training\n",
      "\n",
      "Test set in round158: Average loss: 0.4383, Accuracy: 8876/10020 (88.58%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.371884\n",
      "After training\n",
      "\n",
      "Test set in round159: Average loss: 0.4364, Accuracy: 8883/10020 (88.65%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.485326\n",
      "After training\n",
      "\n",
      "Test set in round160: Average loss: 0.4341, Accuracy: 8882/10020 (88.64%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.310131\n",
      "After training\n",
      "\n",
      "Test set in round161: Average loss: 0.4312, Accuracy: 8886/10020 (88.68%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.462879\n",
      "After training\n",
      "\n",
      "Test set in round162: Average loss: 0.4283, Accuracy: 8890/10020 (88.72%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.307684\n",
      "After training\n",
      "\n",
      "Test set in round163: Average loss: 0.4261, Accuracy: 8902/10020 (88.84%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.444129\n",
      "After training\n",
      "\n",
      "Test set in round164: Average loss: 0.4238, Accuracy: 8902/10020 (88.84%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.382738\n",
      "After training\n",
      "\n",
      "Test set in round165: Average loss: 0.4208, Accuracy: 8893/10020 (88.75%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.380085\n",
      "After training\n",
      "\n",
      "Test set in round166: Average loss: 0.4187, Accuracy: 8902/10020 (88.84%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.377486\n",
      "After training\n",
      "\n",
      "Test set in round167: Average loss: 0.4169, Accuracy: 8896/10020 (88.78%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.434475\n",
      "After training\n",
      "\n",
      "Test set in round168: Average loss: 0.4155, Accuracy: 8905/10020 (88.87%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.375811\n",
      "After training\n",
      "\n",
      "Test set in round169: Average loss: 0.4125, Accuracy: 8919/10020 (89.01%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.389737\n",
      "After training\n",
      "\n",
      "Test set in round170: Average loss: 0.4104, Accuracy: 8913/10020 (88.95%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.387108\n",
      "After training\n",
      "\n",
      "Test set in round171: Average loss: 0.4086, Accuracy: 8926/10020 (89.08%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.433481\n",
      "After training\n",
      "\n",
      "Test set in round172: Average loss: 0.4062, Accuracy: 8931/10020 (89.13%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.400696\n",
      "After training\n",
      "\n",
      "Test set in round173: Average loss: 0.4047, Accuracy: 8930/10020 (89.12%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.428136\n",
      "After training\n",
      "\n",
      "Test set in round174: Average loss: 0.4033, Accuracy: 8932/10020 (89.14%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.473393\n",
      "After training\n",
      "\n",
      "Test set in round175: Average loss: 0.4017, Accuracy: 8936/10020 (89.18%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.438959\n",
      "After training\n",
      "\n",
      "Test set in round176: Average loss: 0.3988, Accuracy: 8936/10020 (89.18%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.283627\n",
      "After training\n",
      "\n",
      "Test set in round177: Average loss: 0.3970, Accuracy: 8940/10020 (89.22%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.486603\n",
      "After training\n",
      "\n",
      "Test set in round178: Average loss: 0.3950, Accuracy: 8944/10020 (89.26%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.337957\n",
      "After training\n",
      "\n",
      "Test set in round179: Average loss: 0.3929, Accuracy: 8935/10020 (89.17%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.463429\n",
      "After training\n",
      "\n",
      "Test set in round180: Average loss: 0.3917, Accuracy: 8932/10020 (89.14%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.386417\n",
      "After training\n",
      "\n",
      "Test set in round181: Average loss: 0.3899, Accuracy: 8945/10020 (89.27%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.417523\n",
      "After training\n",
      "\n",
      "Test set in round182: Average loss: 0.3879, Accuracy: 8954/10020 (89.36%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.365861\n",
      "After training\n",
      "\n",
      "Test set in round183: Average loss: 0.3860, Accuracy: 8962/10020 (89.44%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.413320\n",
      "After training\n",
      "\n",
      "Test set in round184: Average loss: 0.3845, Accuracy: 8971/10020 (89.53%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.379442\n",
      "After training\n",
      "\n",
      "Test set in round185: Average loss: 0.3836, Accuracy: 8960/10020 (89.42%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.401858\n",
      "After training\n",
      "\n",
      "Test set in round186: Average loss: 0.3820, Accuracy: 8955/10020 (89.37%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.408028\n",
      "After training\n",
      "\n",
      "Test set in round187: Average loss: 0.3802, Accuracy: 8969/10020 (89.51%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.397688\n",
      "After training\n",
      "\n",
      "Test set in round188: Average loss: 0.3794, Accuracy: 8964/10020 (89.46%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.326943\n",
      "After training\n",
      "\n",
      "Test set in round189: Average loss: 0.3779, Accuracy: 8965/10020 (89.47%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.420839\n",
      "After training\n",
      "\n",
      "Test set in round190: Average loss: 0.3753, Accuracy: 8979/10020 (89.61%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.322532\n",
      "After training\n",
      "\n",
      "Test set in round191: Average loss: 0.3743, Accuracy: 8960/10020 (89.42%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.253034\n",
      "After training\n",
      "\n",
      "Test set in round192: Average loss: 0.3733, Accuracy: 8971/10020 (89.53%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.251319\n",
      "After training\n",
      "\n",
      "Test set in round193: Average loss: 0.3716, Accuracy: 8988/10020 (89.70%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.416756\n",
      "After training\n",
      "\n",
      "Test set in round194: Average loss: 0.3697, Accuracy: 8985/10020 (89.67%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.318421\n",
      "After training\n",
      "\n",
      "Test set in round195: Average loss: 0.3681, Accuracy: 8985/10020 (89.67%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.422169\n",
      "After training\n",
      "\n",
      "Test set in round196: Average loss: 0.3668, Accuracy: 8989/10020 (89.71%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.249838\n",
      "After training\n",
      "\n",
      "Test set in round197: Average loss: 0.3662, Accuracy: 8972/10020 (89.54%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.434689\n",
      "After training\n",
      "\n",
      "Test set in round198: Average loss: 0.3650, Accuracy: 8967/10020 (89.49%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.316259\n",
      "After training\n",
      "\n",
      "Test set in round199: Average loss: 0.3632, Accuracy: 8989/10020 (89.71%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.312392\n",
      "After training\n",
      "\n",
      "Test set in round200: Average loss: 0.3618, Accuracy: 8998/10020 (89.80%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.243362\n",
      "After training\n",
      "\n",
      "Test set in round201: Average loss: 0.3607, Accuracy: 9003/10020 (89.85%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.386435\n",
      "After training\n",
      "\n",
      "Test set in round202: Average loss: 0.3589, Accuracy: 9011/10020 (89.93%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.243054\n",
      "After training\n",
      "\n",
      "Test set in round203: Average loss: 0.3579, Accuracy: 9026/10020 (90.08%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.402033\n",
      "After training\n",
      "\n",
      "Test set in round204: Average loss: 0.3567, Accuracy: 9014/10020 (89.96%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.306478\n",
      "After training\n",
      "\n",
      "Test set in round205: Average loss: 0.3563, Accuracy: 9013/10020 (89.95%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.320604\n",
      "After training\n",
      "\n",
      "Test set in round206: Average loss: 0.3547, Accuracy: 9008/10020 (89.90%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.375088\n",
      "After training\n",
      "\n",
      "Test set in round207: Average loss: 0.3540, Accuracy: 9004/10020 (89.86%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.368432\n",
      "After training\n",
      "\n",
      "Test set in round208: Average loss: 0.3525, Accuracy: 9018/10020 (90.00%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.312047\n",
      "After training\n",
      "\n",
      "Test set in round209: Average loss: 0.3512, Accuracy: 9022/10020 (90.04%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.406231\n",
      "After training\n",
      "\n",
      "Test set in round210: Average loss: 0.3499, Accuracy: 9030/10020 (90.12%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.394516\n",
      "After training\n",
      "\n",
      "Test set in round211: Average loss: 0.3488, Accuracy: 9025/10020 (90.07%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.324348\n",
      "After training\n",
      "\n",
      "Test set in round212: Average loss: 0.3473, Accuracy: 9036/10020 (90.18%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.360532\n",
      "After training\n",
      "\n",
      "Test set in round213: Average loss: 0.3463, Accuracy: 9032/10020 (90.14%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.228500\n",
      "After training\n",
      "\n",
      "Test set in round214: Average loss: 0.3457, Accuracy: 9035/10020 (90.17%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.372759\n",
      "After training\n",
      "\n",
      "Test set in round215: Average loss: 0.3442, Accuracy: 9030/10020 (90.12%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.318958\n",
      "After training\n",
      "\n",
      "Test set in round216: Average loss: 0.3438, Accuracy: 9036/10020 (90.18%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.400472\n",
      "After training\n",
      "\n",
      "Test set in round217: Average loss: 0.3427, Accuracy: 9040/10020 (90.22%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.304778\n",
      "After training\n",
      "\n",
      "Test set in round218: Average loss: 0.3414, Accuracy: 9042/10020 (90.24%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.362681\n",
      "After training\n",
      "\n",
      "Test set in round219: Average loss: 0.3407, Accuracy: 9043/10020 (90.25%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.346820\n",
      "After training\n",
      "\n",
      "Test set in round220: Average loss: 0.3390, Accuracy: 9048/10020 (90.30%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.349242\n",
      "After training\n",
      "\n",
      "Test set in round221: Average loss: 0.3379, Accuracy: 9060/10020 (90.42%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.411338\n",
      "After training\n",
      "\n",
      "Test set in round222: Average loss: 0.3369, Accuracy: 9054/10020 (90.36%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.293576\n",
      "After training\n",
      "\n",
      "Test set in round223: Average loss: 0.3372, Accuracy: 9042/10020 (90.24%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.393883\n",
      "After training\n",
      "\n",
      "Test set in round224: Average loss: 0.3350, Accuracy: 9056/10020 (90.38%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.217106\n",
      "After training\n",
      "\n",
      "Test set in round225: Average loss: 0.3341, Accuracy: 9064/10020 (90.46%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.334934\n",
      "After training\n",
      "\n",
      "Test set in round226: Average loss: 0.3336, Accuracy: 9056/10020 (90.38%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.210938\n",
      "After training\n",
      "\n",
      "Test set in round227: Average loss: 0.3329, Accuracy: 9058/10020 (90.40%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.396597\n",
      "After training\n",
      "\n",
      "Test set in round228: Average loss: 0.3321, Accuracy: 9046/10020 (90.28%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.292985\n",
      "After training\n",
      "\n",
      "Test set in round229: Average loss: 0.3310, Accuracy: 9054/10020 (90.36%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.303982\n",
      "After training\n",
      "\n",
      "Test set in round230: Average loss: 0.3298, Accuracy: 9065/10020 (90.47%)\n",
      "\n",
      "Train Epoch: 0 [6000/6000 (0%)]\tLoss: 0.359609\n",
      "After training\n",
      "\n",
      "Test set in round231: Average loss: 0.3291, Accuracy: 9060/10020 (90.42%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "logdir = '/root/notebooks/tensorflow/logs/pysyft_iidFedSGD'\n",
    "writer = SummaryWriter(logdir)\n",
    "\n",
    "model_list = []\n",
    "model_list = model_init(workers, Net().to(device))\n",
    "opt_list = opt_init(model_list)\n",
    "# not finish in train, finish latter\n",
    "pars = [list(model.parameters()) for model in model_list]\n",
    "\n",
    "for r in range(1, args.rounds + 1):\n",
    "    train(args, device, train_loader, opt_list, workers)\n",
    "    print(\"After training\")\n",
    "    test(args, model_list[0], device, test_loader, r)\n",
    "\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
