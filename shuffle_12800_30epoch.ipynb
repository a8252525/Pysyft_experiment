{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 30\n",
    "n_train_items = 12800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Torch was already hooked... skipping hooking process\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "\n",
    "workers = connect_to_workers(n_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ]))\n",
    "#     .federate(workers), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "less_train_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "less_test_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy \n",
    "# #mnist_dataset.__getitem__(2)[1]\n",
    "# a = (mnist_dataset.__getitem__(0)[0]).numpy()\n",
    "# a.dtype = 'uint8'\n",
    "# print(a)\n",
    "# Image.fromarray(a[0], mode= 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_init(workers, Net):\n",
    "    model_list = list()\n",
    "    for worker in workers:\n",
    "        model_list.append(Net)\n",
    "    return model_list\n",
    "def opt_init(model_list):\n",
    "    opt_list = list()\n",
    "    for model  in model_list:\n",
    "        opt_list.append(optim.SGD(model.parameters(), lr=args.lr))\n",
    "    return opt_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device, less_train_dataloader, opt_list, epoch, workers):\n",
    "    global model_list\n",
    "    ## start training and record the model into model_list\n",
    "    \n",
    "    for batch_idx, (data, target) in enumerate(less_train_dataloader): # <-- now it is a distributed dataset\n",
    "        model_on_worker = model_list[batch_idx%len(workers)]\n",
    "        model_on_worker.train()\n",
    "        model_on_worker.send(workers[batch_idx%len(workers)]) # <-- NEW: send the model to the right location\n",
    "        \n",
    "        data_on_worker = data.send(workers[batch_idx%len(workers)])\n",
    "        target_on_worker = target.send(workers[batch_idx%len(workers)])\n",
    "        \n",
    "        data_on_worker, target_on_worker = data_on_worker.to(device), target_on_worker.to(device)\n",
    "        \n",
    "        opt_list[batch_idx%len(workers)].zero_grad()\n",
    "        \n",
    "        output = model_on_worker(data_on_worker)\n",
    "        loss = F.nll_loss(output, target_on_worker)\n",
    "        loss.backward()\n",
    "        \n",
    "        opt_list[batch_idx%len(workers)].step()\n",
    "        model_on_worker.get() # <-- NEW: get the model back\n",
    "        \n",
    "        model_list[batch_idx%len(workers)] = model_on_worker #When len(dataloader) is longer than the len(worker) send and get must be modified\n",
    "        #model_list here is full of the model which has trained on the workers, there are all different now.\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(less_train_dataloader) * args.batch_size,\n",
    "                100. * batch_idx / len(less_train_dataloader), loss.item()))\n",
    "\n",
    "\n",
    "    ##Aggregation time\n",
    "    new_model = []\n",
    "    tmp_model = Net().to(device)\n",
    "    with torch.no_grad():\n",
    "        for p in model_list[0].parameters():\n",
    "            new_model.append(0)\n",
    "            \n",
    "        for m in model_list:\n",
    "            for par_idx, par in enumerate(m.parameters()):\n",
    "                #average the model_list\n",
    "                new_model[par_idx] = new_model[par_idx]+par.data\n",
    "                # we get new model in list format and need to set_ to model\n",
    "        for worker in range(len(workers)):\n",
    "            for par_idx in range(len(new_model)):\n",
    "                list(model_list[worker].parameters())[par_idx].set_(new_model[par_idx]/len(workers))\n",
    "        #init model with new_model\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "for i,j in enumerate([5,5]):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader*args.batch_size)\n",
    "    #Since the test loader here is a list, we can get the len by * it with batch.size\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader* args.batch_size),\n",
    "        100. * correct / (len(test_loader)*args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/12800 (0%)]\tLoss: 2.288257\n",
      "Train Epoch: 1 [1920/12800 (15%)]\tLoss: 2.178536\n",
      "Train Epoch: 1 [3840/12800 (30%)]\tLoss: 1.928699\n",
      "Train Epoch: 1 [5760/12800 (45%)]\tLoss: 1.526911\n",
      "Train Epoch: 1 [7680/12800 (60%)]\tLoss: 0.991821\n",
      "Train Epoch: 1 [9600/12800 (75%)]\tLoss: 0.653335\n",
      "Train Epoch: 1 [11520/12800 (90%)]\tLoss: 0.655600\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.5874, Accuracy: 8152/10048 (81%)\n",
      "\n",
      "Train Epoch: 2 [0/12800 (0%)]\tLoss: 0.582307\n",
      "Train Epoch: 2 [1920/12800 (15%)]\tLoss: 0.349787\n",
      "Train Epoch: 2 [3840/12800 (30%)]\tLoss: 0.244426\n",
      "Train Epoch: 2 [5760/12800 (45%)]\tLoss: 0.367783\n",
      "Train Epoch: 2 [7680/12800 (60%)]\tLoss: 0.369737\n",
      "Train Epoch: 2 [9600/12800 (75%)]\tLoss: 0.296277\n",
      "Train Epoch: 2 [11520/12800 (90%)]\tLoss: 0.511333\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.3229, Accuracy: 8989/10048 (89%)\n",
      "\n",
      "Train Epoch: 3 [0/12800 (0%)]\tLoss: 0.308202\n",
      "Train Epoch: 3 [1920/12800 (15%)]\tLoss: 0.194156\n",
      "Train Epoch: 3 [3840/12800 (30%)]\tLoss: 0.151043\n",
      "Train Epoch: 3 [5760/12800 (45%)]\tLoss: 0.230129\n",
      "Train Epoch: 3 [7680/12800 (60%)]\tLoss: 0.269391\n",
      "Train Epoch: 3 [9600/12800 (75%)]\tLoss: 0.206813\n",
      "Train Epoch: 3 [11520/12800 (90%)]\tLoss: 0.422838\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.2439, Accuracy: 9257/10048 (92%)\n",
      "\n",
      "Train Epoch: 4 [0/12800 (0%)]\tLoss: 0.202282\n",
      "Train Epoch: 4 [1920/12800 (15%)]\tLoss: 0.157734\n",
      "Train Epoch: 4 [3840/12800 (30%)]\tLoss: 0.102655\n",
      "Train Epoch: 4 [5760/12800 (45%)]\tLoss: 0.160942\n",
      "Train Epoch: 4 [7680/12800 (60%)]\tLoss: 0.212174\n",
      "Train Epoch: 4 [9600/12800 (75%)]\tLoss: 0.155351\n",
      "Train Epoch: 4 [11520/12800 (90%)]\tLoss: 0.348821\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.2012, Accuracy: 9392/10048 (93%)\n",
      "\n",
      "Train Epoch: 5 [0/12800 (0%)]\tLoss: 0.145167\n",
      "Train Epoch: 5 [1920/12800 (15%)]\tLoss: 0.133413\n",
      "Train Epoch: 5 [3840/12800 (30%)]\tLoss: 0.071949\n",
      "Train Epoch: 5 [5760/12800 (45%)]\tLoss: 0.125486\n",
      "Train Epoch: 5 [7680/12800 (60%)]\tLoss: 0.178245\n",
      "Train Epoch: 5 [9600/12800 (75%)]\tLoss: 0.127501\n",
      "Train Epoch: 5 [11520/12800 (90%)]\tLoss: 0.291881\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.1727, Accuracy: 9484/10048 (94%)\n",
      "\n",
      "Train Epoch: 6 [0/12800 (0%)]\tLoss: 0.112629\n",
      "Train Epoch: 6 [1920/12800 (15%)]\tLoss: 0.115408\n",
      "Train Epoch: 6 [3840/12800 (30%)]\tLoss: 0.052245\n",
      "Train Epoch: 6 [5760/12800 (45%)]\tLoss: 0.107393\n",
      "Train Epoch: 6 [7680/12800 (60%)]\tLoss: 0.154989\n",
      "Train Epoch: 6 [9600/12800 (75%)]\tLoss: 0.112789\n",
      "Train Epoch: 6 [11520/12800 (90%)]\tLoss: 0.242780\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.1529, Accuracy: 9543/10048 (95%)\n",
      "\n",
      "Train Epoch: 7 [0/12800 (0%)]\tLoss: 0.092523\n",
      "Train Epoch: 7 [1920/12800 (15%)]\tLoss: 0.103227\n",
      "Train Epoch: 7 [3840/12800 (30%)]\tLoss: 0.038568\n",
      "Train Epoch: 7 [5760/12800 (45%)]\tLoss: 0.095863\n",
      "Train Epoch: 7 [7680/12800 (60%)]\tLoss: 0.139749\n",
      "Train Epoch: 7 [9600/12800 (75%)]\tLoss: 0.101544\n",
      "Train Epoch: 7 [11520/12800 (90%)]\tLoss: 0.204378\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.1372, Accuracy: 9590/10048 (95%)\n",
      "\n",
      "Train Epoch: 8 [0/12800 (0%)]\tLoss: 0.077776\n",
      "Train Epoch: 8 [1920/12800 (15%)]\tLoss: 0.096038\n",
      "Train Epoch: 8 [3840/12800 (30%)]\tLoss: 0.029370\n",
      "Train Epoch: 8 [5760/12800 (45%)]\tLoss: 0.090624\n",
      "Train Epoch: 8 [7680/12800 (60%)]\tLoss: 0.127198\n",
      "Train Epoch: 8 [9600/12800 (75%)]\tLoss: 0.092253\n",
      "Train Epoch: 8 [11520/12800 (90%)]\tLoss: 0.174334\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.1239, Accuracy: 9625/10048 (96%)\n",
      "\n",
      "Train Epoch: 9 [0/12800 (0%)]\tLoss: 0.067912\n",
      "Train Epoch: 9 [1920/12800 (15%)]\tLoss: 0.089181\n",
      "Train Epoch: 9 [3840/12800 (30%)]\tLoss: 0.022991\n",
      "Train Epoch: 9 [5760/12800 (45%)]\tLoss: 0.086255\n",
      "Train Epoch: 9 [7680/12800 (60%)]\tLoss: 0.117653\n",
      "Train Epoch: 9 [9600/12800 (75%)]\tLoss: 0.082854\n",
      "Train Epoch: 9 [11520/12800 (90%)]\tLoss: 0.148216\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.1132, Accuracy: 9661/10048 (96%)\n",
      "\n",
      "Train Epoch: 10 [0/12800 (0%)]\tLoss: 0.060577\n",
      "Train Epoch: 10 [1920/12800 (15%)]\tLoss: 0.083321\n",
      "Train Epoch: 10 [3840/12800 (30%)]\tLoss: 0.018529\n",
      "Train Epoch: 10 [5760/12800 (45%)]\tLoss: 0.085224\n",
      "Train Epoch: 10 [7680/12800 (60%)]\tLoss: 0.110563\n",
      "Train Epoch: 10 [9600/12800 (75%)]\tLoss: 0.075141\n",
      "Train Epoch: 10 [11520/12800 (90%)]\tLoss: 0.126485\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.1046, Accuracy: 9685/10048 (96%)\n",
      "\n",
      "Train Epoch: 11 [0/12800 (0%)]\tLoss: 0.054199\n",
      "Train Epoch: 11 [1920/12800 (15%)]\tLoss: 0.078779\n",
      "Train Epoch: 11 [3840/12800 (30%)]\tLoss: 0.015383\n",
      "Train Epoch: 11 [5760/12800 (45%)]\tLoss: 0.084802\n",
      "Train Epoch: 11 [7680/12800 (60%)]\tLoss: 0.105003\n",
      "Train Epoch: 11 [9600/12800 (75%)]\tLoss: 0.069195\n",
      "Train Epoch: 11 [11520/12800 (90%)]\tLoss: 0.107227\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0977, Accuracy: 9706/10048 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/12800 (0%)]\tLoss: 0.048561\n",
      "Train Epoch: 12 [1920/12800 (15%)]\tLoss: 0.074354\n",
      "Train Epoch: 12 [3840/12800 (30%)]\tLoss: 0.013259\n",
      "Train Epoch: 12 [5760/12800 (45%)]\tLoss: 0.084635\n",
      "Train Epoch: 12 [7680/12800 (60%)]\tLoss: 0.099850\n",
      "Train Epoch: 12 [9600/12800 (75%)]\tLoss: 0.063876\n",
      "Train Epoch: 12 [11520/12800 (90%)]\tLoss: 0.091624\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0923, Accuracy: 9719/10048 (97%)\n",
      "\n",
      "Train Epoch: 13 [0/12800 (0%)]\tLoss: 0.044591\n",
      "Train Epoch: 13 [1920/12800 (15%)]\tLoss: 0.071059\n",
      "Train Epoch: 13 [3840/12800 (30%)]\tLoss: 0.011685\n",
      "Train Epoch: 13 [5760/12800 (45%)]\tLoss: 0.084174\n",
      "Train Epoch: 13 [7680/12800 (60%)]\tLoss: 0.094151\n",
      "Train Epoch: 13 [9600/12800 (75%)]\tLoss: 0.058691\n",
      "Train Epoch: 13 [11520/12800 (90%)]\tLoss: 0.077463\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0874, Accuracy: 9731/10048 (97%)\n",
      "\n",
      "Train Epoch: 14 [0/12800 (0%)]\tLoss: 0.040566\n",
      "Train Epoch: 14 [1920/12800 (15%)]\tLoss: 0.068840\n",
      "Train Epoch: 14 [3840/12800 (30%)]\tLoss: 0.010403\n",
      "Train Epoch: 14 [5760/12800 (45%)]\tLoss: 0.083810\n",
      "Train Epoch: 14 [7680/12800 (60%)]\tLoss: 0.090011\n",
      "Train Epoch: 14 [9600/12800 (75%)]\tLoss: 0.055041\n",
      "Train Epoch: 14 [11520/12800 (90%)]\tLoss: 0.065507\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0830, Accuracy: 9740/10048 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/12800 (0%)]\tLoss: 0.036647\n",
      "Train Epoch: 15 [1920/12800 (15%)]\tLoss: 0.067678\n",
      "Train Epoch: 15 [3840/12800 (30%)]\tLoss: 0.009552\n",
      "Train Epoch: 15 [5760/12800 (45%)]\tLoss: 0.083280\n",
      "Train Epoch: 15 [7680/12800 (60%)]\tLoss: 0.086322\n",
      "Train Epoch: 15 [9600/12800 (75%)]\tLoss: 0.051655\n",
      "Train Epoch: 15 [11520/12800 (90%)]\tLoss: 0.055428\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0789, Accuracy: 9750/10048 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/12800 (0%)]\tLoss: 0.033354\n",
      "Train Epoch: 16 [1920/12800 (15%)]\tLoss: 0.068267\n",
      "Train Epoch: 16 [3840/12800 (30%)]\tLoss: 0.008810\n",
      "Train Epoch: 16 [5760/12800 (45%)]\tLoss: 0.082035\n",
      "Train Epoch: 16 [7680/12800 (60%)]\tLoss: 0.082453\n",
      "Train Epoch: 16 [9600/12800 (75%)]\tLoss: 0.048527\n",
      "Train Epoch: 16 [11520/12800 (90%)]\tLoss: 0.046411\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0757, Accuracy: 9762/10048 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/12800 (0%)]\tLoss: 0.030746\n",
      "Train Epoch: 17 [1920/12800 (15%)]\tLoss: 0.069394\n",
      "Train Epoch: 17 [3840/12800 (30%)]\tLoss: 0.008123\n",
      "Train Epoch: 17 [5760/12800 (45%)]\tLoss: 0.080043\n",
      "Train Epoch: 17 [7680/12800 (60%)]\tLoss: 0.078502\n",
      "Train Epoch: 17 [9600/12800 (75%)]\tLoss: 0.046186\n",
      "Train Epoch: 17 [11520/12800 (90%)]\tLoss: 0.038558\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0728, Accuracy: 9766/10048 (97%)\n",
      "\n",
      "Train Epoch: 18 [0/12800 (0%)]\tLoss: 0.028401\n",
      "Train Epoch: 18 [1920/12800 (15%)]\tLoss: 0.069827\n",
      "Train Epoch: 18 [3840/12800 (30%)]\tLoss: 0.007574\n",
      "Train Epoch: 18 [5760/12800 (45%)]\tLoss: 0.079053\n",
      "Train Epoch: 18 [7680/12800 (60%)]\tLoss: 0.073976\n",
      "Train Epoch: 18 [9600/12800 (75%)]\tLoss: 0.043754\n",
      "Train Epoch: 18 [11520/12800 (90%)]\tLoss: 0.032436\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0704, Accuracy: 9769/10048 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/12800 (0%)]\tLoss: 0.025815\n",
      "Train Epoch: 19 [1920/12800 (15%)]\tLoss: 0.070008\n",
      "Train Epoch: 19 [3840/12800 (30%)]\tLoss: 0.007122\n",
      "Train Epoch: 19 [5760/12800 (45%)]\tLoss: 0.077201\n",
      "Train Epoch: 19 [7680/12800 (60%)]\tLoss: 0.070426\n",
      "Train Epoch: 19 [9600/12800 (75%)]\tLoss: 0.040940\n",
      "Train Epoch: 19 [11520/12800 (90%)]\tLoss: 0.027124\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0685, Accuracy: 9773/10048 (97%)\n",
      "\n",
      "Train Epoch: 20 [0/12800 (0%)]\tLoss: 0.024058\n",
      "Train Epoch: 20 [1920/12800 (15%)]\tLoss: 0.071306\n",
      "Train Epoch: 20 [3840/12800 (30%)]\tLoss: 0.006675\n",
      "Train Epoch: 20 [5760/12800 (45%)]\tLoss: 0.075891\n",
      "Train Epoch: 20 [7680/12800 (60%)]\tLoss: 0.065914\n",
      "Train Epoch: 20 [9600/12800 (75%)]\tLoss: 0.038391\n",
      "Train Epoch: 20 [11520/12800 (90%)]\tLoss: 0.022832\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0668, Accuracy: 9780/10048 (97%)\n",
      "\n",
      "Train Epoch: 21 [0/12800 (0%)]\tLoss: 0.022393\n",
      "Train Epoch: 21 [1920/12800 (15%)]\tLoss: 0.071444\n",
      "Train Epoch: 21 [3840/12800 (30%)]\tLoss: 0.006215\n",
      "Train Epoch: 21 [5760/12800 (45%)]\tLoss: 0.074567\n",
      "Train Epoch: 21 [7680/12800 (60%)]\tLoss: 0.061974\n",
      "Train Epoch: 21 [9600/12800 (75%)]\tLoss: 0.036472\n",
      "Train Epoch: 21 [11520/12800 (90%)]\tLoss: 0.019295\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0653, Accuracy: 9784/10048 (97%)\n",
      "\n",
      "Train Epoch: 22 [0/12800 (0%)]\tLoss: 0.020593\n",
      "Train Epoch: 22 [1920/12800 (15%)]\tLoss: 0.071930\n",
      "Train Epoch: 22 [3840/12800 (30%)]\tLoss: 0.005831\n",
      "Train Epoch: 22 [5760/12800 (45%)]\tLoss: 0.073184\n",
      "Train Epoch: 22 [7680/12800 (60%)]\tLoss: 0.058390\n",
      "Train Epoch: 22 [9600/12800 (75%)]\tLoss: 0.035020\n",
      "Train Epoch: 22 [11520/12800 (90%)]\tLoss: 0.016217\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0642, Accuracy: 9786/10048 (97%)\n",
      "\n",
      "Train Epoch: 23 [0/12800 (0%)]\tLoss: 0.019496\n",
      "Train Epoch: 23 [1920/12800 (15%)]\tLoss: 0.072752\n",
      "Train Epoch: 23 [3840/12800 (30%)]\tLoss: 0.005468\n",
      "Train Epoch: 23 [5760/12800 (45%)]\tLoss: 0.071408\n",
      "Train Epoch: 23 [7680/12800 (60%)]\tLoss: 0.054522\n",
      "Train Epoch: 23 [9600/12800 (75%)]\tLoss: 0.033206\n",
      "Train Epoch: 23 [11520/12800 (90%)]\tLoss: 0.013967\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0629, Accuracy: 9795/10048 (97%)\n",
      "\n",
      "Train Epoch: 24 [0/12800 (0%)]\tLoss: 0.018013\n",
      "Train Epoch: 24 [1920/12800 (15%)]\tLoss: 0.072275\n",
      "Train Epoch: 24 [3840/12800 (30%)]\tLoss: 0.005154\n",
      "Train Epoch: 24 [5760/12800 (45%)]\tLoss: 0.069730\n",
      "Train Epoch: 24 [7680/12800 (60%)]\tLoss: 0.051317\n",
      "Train Epoch: 24 [9600/12800 (75%)]\tLoss: 0.032059\n",
      "Train Epoch: 24 [11520/12800 (90%)]\tLoss: 0.012065\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0620, Accuracy: 9796/10048 (97%)\n",
      "\n",
      "Train Epoch: 25 [0/12800 (0%)]\tLoss: 0.016990\n",
      "Train Epoch: 25 [1920/12800 (15%)]\tLoss: 0.072462\n",
      "Train Epoch: 25 [3840/12800 (30%)]\tLoss: 0.004865\n",
      "Train Epoch: 25 [5760/12800 (45%)]\tLoss: 0.066962\n",
      "Train Epoch: 25 [7680/12800 (60%)]\tLoss: 0.048141\n",
      "Train Epoch: 25 [9600/12800 (75%)]\tLoss: 0.029874\n",
      "Train Epoch: 25 [11520/12800 (90%)]\tLoss: 0.010565\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0610, Accuracy: 9799/10048 (98%)\n",
      "\n",
      "Train Epoch: 26 [0/12800 (0%)]\tLoss: 0.015828\n",
      "Train Epoch: 26 [1920/12800 (15%)]\tLoss: 0.072211\n",
      "Train Epoch: 26 [3840/12800 (30%)]\tLoss: 0.004558\n",
      "Train Epoch: 26 [5760/12800 (45%)]\tLoss: 0.064484\n",
      "Train Epoch: 26 [7680/12800 (60%)]\tLoss: 0.044742\n",
      "Train Epoch: 26 [9600/12800 (75%)]\tLoss: 0.028225\n",
      "Train Epoch: 26 [11520/12800 (90%)]\tLoss: 0.009297\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0602, Accuracy: 9800/10048 (98%)\n",
      "\n",
      "Train Epoch: 27 [0/12800 (0%)]\tLoss: 0.014850\n",
      "Train Epoch: 27 [1920/12800 (15%)]\tLoss: 0.071463\n",
      "Train Epoch: 27 [3840/12800 (30%)]\tLoss: 0.004286\n",
      "Train Epoch: 27 [5760/12800 (45%)]\tLoss: 0.062184\n",
      "Train Epoch: 27 [7680/12800 (60%)]\tLoss: 0.041183\n",
      "Train Epoch: 27 [9600/12800 (75%)]\tLoss: 0.027031\n",
      "Train Epoch: 27 [11520/12800 (90%)]\tLoss: 0.008179\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0595, Accuracy: 9808/10048 (98%)\n",
      "\n",
      "Train Epoch: 28 [0/12800 (0%)]\tLoss: 0.014030\n",
      "Train Epoch: 28 [1920/12800 (15%)]\tLoss: 0.071022\n",
      "Train Epoch: 28 [3840/12800 (30%)]\tLoss: 0.004045\n",
      "Train Epoch: 28 [5760/12800 (45%)]\tLoss: 0.059058\n",
      "Train Epoch: 28 [7680/12800 (60%)]\tLoss: 0.037702\n",
      "Train Epoch: 28 [9600/12800 (75%)]\tLoss: 0.026008\n",
      "Train Epoch: 28 [11520/12800 (90%)]\tLoss: 0.007257\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0588, Accuracy: 9813/10048 (98%)\n",
      "\n",
      "Train Epoch: 29 [0/12800 (0%)]\tLoss: 0.013224\n",
      "Train Epoch: 29 [1920/12800 (15%)]\tLoss: 0.070496\n",
      "Train Epoch: 29 [3840/12800 (30%)]\tLoss: 0.003887\n",
      "Train Epoch: 29 [5760/12800 (45%)]\tLoss: 0.055535\n",
      "Train Epoch: 29 [7680/12800 (60%)]\tLoss: 0.034719\n",
      "Train Epoch: 29 [9600/12800 (75%)]\tLoss: 0.024980\n",
      "Train Epoch: 29 [11520/12800 (90%)]\tLoss: 0.006550\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0581, Accuracy: 9816/10048 (98%)\n",
      "\n",
      "Train Epoch: 30 [0/12800 (0%)]\tLoss: 0.012574\n",
      "Train Epoch: 30 [1920/12800 (15%)]\tLoss: 0.071053\n",
      "Train Epoch: 30 [3840/12800 (30%)]\tLoss: 0.003639\n",
      "Train Epoch: 30 [5760/12800 (45%)]\tLoss: 0.052262\n",
      "Train Epoch: 30 [7680/12800 (60%)]\tLoss: 0.031591\n",
      "Train Epoch: 30 [9600/12800 (75%)]\tLoss: 0.023597\n",
      "Train Epoch: 30 [11520/12800 (90%)]\tLoss: 0.005840\n",
      "After training\n",
      "\n",
      "Test set: Average loss: 0.0576, Accuracy: 9823/10048 (98%)\n",
      "\n",
      "CPU times: user 4min 26s, sys: 2.87 s, total: 4min 29s\n",
      "Wall time: 4min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "model_list = []\n",
    "model_list = model_init(workers, Net().to(device))\n",
    "opt_list = opt_init(model_list)\n",
    "# not finish in train, finish latter\n",
    "pars = [list(model.parameters()) for model in model_list]\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, device, less_train_dataloader, opt_list, epoch, workers)\n",
    "    print(\"After training\")\n",
    "    test(args, model_list[0], device, less_test_dataloader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
