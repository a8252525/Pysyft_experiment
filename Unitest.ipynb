{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "n_train_items = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "\n",
    "workers = connect_to_workers(n_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 3\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "da5f800e150b40068ad51ea7a7dcb753",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "adb31e6a9ab4497a89bf7a19bbd76c38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2bc18a2e31f413cbda37eec96f474c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../data/MNIST/raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3260f97c572464c93a078ae68c14884",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../data/MNIST/raw\n",
      "Processing...\n",
      "Done!\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ]))\n",
    "#     .federate(workers), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "less_train_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "less_test_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy \n",
    "# #mnist_dataset.__getitem__(2)[1]\n",
    "# a = (mnist_dataset.__getitem__(0)[0]).numpy()\n",
    "# a.dtype = 'uint8'\n",
    "# print(a)\n",
    "# Image.fromarray(a[0], mode= 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, less_train_dataloader, optimizer, epoch, workers):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(less_train_dataloader): # <-- now it is a distributed dataset\n",
    "        model.send(workers[batch_idx%len(workers)]) # <-- NEW: send the model to the right location\n",
    "        \n",
    "        data_on_worker = data.send(workers[batch_idx%len(workers)])\n",
    "        target_on_worker = target.send(workers[batch_idx%len(workers)])\n",
    "        \n",
    "        data_on_worker, target_on_worker = data_on_worker.to(device), target_on_worker.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_on_worker)\n",
    "        loss = F.nll_loss(output, target_on_worker)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(less_train_dataloader) * args.batch_size,\n",
    "                100. * batch_idx / len(less_train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader*args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader* args.batch_size),\n",
    "        100. * correct / (len(test_loader)*args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1280 (0%)]\tLoss: 2.306381\n",
      "Train Epoch: 1 [192/1280 (15%)]\tLoss: 2.296128\n",
      "Train Epoch: 1 [384/1280 (30%)]\tLoss: 2.294286\n",
      "Train Epoch: 1 [576/1280 (45%)]\tLoss: 2.289335\n",
      "Train Epoch: 1 [768/1280 (60%)]\tLoss: 2.274065\n",
      "Train Epoch: 1 [960/1280 (75%)]\tLoss: 2.269934\n",
      "Train Epoch: 1 [1152/1280 (90%)]\tLoss: 2.266338\n",
      "\n",
      "Test set: Average loss: 2.2552, Accuracy: 430/1280 (34%)\n",
      "\n",
      "Train Epoch: 2 [0/1280 (0%)]\tLoss: 2.244860\n",
      "Train Epoch: 2 [192/1280 (15%)]\tLoss: 2.237944\n",
      "Train Epoch: 2 [384/1280 (30%)]\tLoss: 2.231242\n",
      "Train Epoch: 2 [576/1280 (45%)]\tLoss: 2.233209\n",
      "Train Epoch: 2 [768/1280 (60%)]\tLoss: 2.204953\n",
      "Train Epoch: 2 [960/1280 (75%)]\tLoss: 2.202785\n",
      "Train Epoch: 2 [1152/1280 (90%)]\tLoss: 2.190120\n",
      "\n",
      "Test set: Average loss: 2.1937, Accuracy: 654/1280 (51%)\n",
      "\n",
      "Train Epoch: 3 [0/1280 (0%)]\tLoss: 2.175524\n",
      "Train Epoch: 3 [192/1280 (15%)]\tLoss: 2.160716\n",
      "Train Epoch: 3 [384/1280 (30%)]\tLoss: 2.153394\n",
      "Train Epoch: 3 [576/1280 (45%)]\tLoss: 2.160794\n",
      "Train Epoch: 3 [768/1280 (60%)]\tLoss: 2.112380\n",
      "Train Epoch: 3 [960/1280 (75%)]\tLoss: 2.102863\n",
      "Train Epoch: 3 [1152/1280 (90%)]\tLoss: 2.076547\n",
      "\n",
      "Test set: Average loss: 2.0972, Accuracy: 711/1280 (56%)\n",
      "\n",
      "Train Epoch: 4 [0/1280 (0%)]\tLoss: 2.067452\n",
      "Train Epoch: 4 [192/1280 (15%)]\tLoss: 2.027533\n",
      "Train Epoch: 4 [384/1280 (30%)]\tLoss: 2.023517\n",
      "Train Epoch: 4 [576/1280 (45%)]\tLoss: 2.036726\n",
      "Train Epoch: 4 [768/1280 (60%)]\tLoss: 1.952974\n",
      "Train Epoch: 4 [960/1280 (75%)]\tLoss: 1.924651\n",
      "Train Epoch: 4 [1152/1280 (90%)]\tLoss: 1.877895\n",
      "\n",
      "Test set: Average loss: 1.9212, Accuracy: 768/1280 (60%)\n",
      "\n",
      "Train Epoch: 5 [0/1280 (0%)]\tLoss: 1.869592\n",
      "Train Epoch: 5 [192/1280 (15%)]\tLoss: 1.774895\n",
      "Train Epoch: 5 [384/1280 (30%)]\tLoss: 1.783774\n",
      "Train Epoch: 5 [576/1280 (45%)]\tLoss: 1.805567\n",
      "Train Epoch: 5 [768/1280 (60%)]\tLoss: 1.656358\n",
      "Train Epoch: 5 [960/1280 (75%)]\tLoss: 1.598554\n",
      "Train Epoch: 5 [1152/1280 (90%)]\tLoss: 1.522259\n",
      "\n",
      "Test set: Average loss: 1.6096, Accuracy: 830/1280 (65%)\n",
      "\n",
      "Train Epoch: 6 [0/1280 (0%)]\tLoss: 1.518916\n",
      "Train Epoch: 6 [192/1280 (15%)]\tLoss: 1.351832\n",
      "Train Epoch: 6 [384/1280 (30%)]\tLoss: 1.394816\n",
      "Train Epoch: 6 [576/1280 (45%)]\tLoss: 1.439292\n",
      "Train Epoch: 6 [768/1280 (60%)]\tLoss: 1.217837\n",
      "Train Epoch: 6 [960/1280 (75%)]\tLoss: 1.161871\n",
      "Train Epoch: 6 [1152/1280 (90%)]\tLoss: 1.066616\n",
      "\n",
      "Test set: Average loss: 1.2238, Accuracy: 876/1280 (68%)\n",
      "\n",
      "Train Epoch: 7 [0/1280 (0%)]\tLoss: 1.093125\n",
      "Train Epoch: 7 [192/1280 (15%)]\tLoss: 0.912104\n",
      "Train Epoch: 7 [384/1280 (30%)]\tLoss: 0.989242\n",
      "Train Epoch: 7 [576/1280 (45%)]\tLoss: 1.078019\n",
      "Train Epoch: 7 [768/1280 (60%)]\tLoss: 0.846427\n",
      "Train Epoch: 7 [960/1280 (75%)]\tLoss: 0.837206\n",
      "Train Epoch: 7 [1152/1280 (90%)]\tLoss: 0.721093\n",
      "\n",
      "Test set: Average loss: 0.9445, Accuracy: 940/1280 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/1280 (0%)]\tLoss: 0.800499\n",
      "Train Epoch: 8 [192/1280 (15%)]\tLoss: 0.640076\n",
      "Train Epoch: 8 [384/1280 (30%)]\tLoss: 0.723397\n",
      "Train Epoch: 8 [576/1280 (45%)]\tLoss: 0.842514\n",
      "Train Epoch: 8 [768/1280 (60%)]\tLoss: 0.649190\n",
      "Train Epoch: 8 [960/1280 (75%)]\tLoss: 0.672531\n",
      "Train Epoch: 8 [1152/1280 (90%)]\tLoss: 0.523167\n",
      "\n",
      "Test set: Average loss: 0.7877, Accuracy: 984/1280 (77%)\n",
      "\n",
      "Train Epoch: 9 [0/1280 (0%)]\tLoss: 0.643666\n",
      "Train Epoch: 9 [192/1280 (15%)]\tLoss: 0.508635\n",
      "Train Epoch: 9 [384/1280 (30%)]\tLoss: 0.576087\n",
      "Train Epoch: 9 [576/1280 (45%)]\tLoss: 0.697690\n",
      "Train Epoch: 9 [768/1280 (60%)]\tLoss: 0.542451\n",
      "Train Epoch: 9 [960/1280 (75%)]\tLoss: 0.589961\n",
      "Train Epoch: 9 [1152/1280 (90%)]\tLoss: 0.409992\n",
      "\n",
      "Test set: Average loss: 0.6975, Accuracy: 1000/1280 (78%)\n",
      "\n",
      "Train Epoch: 10 [0/1280 (0%)]\tLoss: 0.557146\n",
      "Train Epoch: 10 [192/1280 (15%)]\tLoss: 0.447836\n",
      "Train Epoch: 10 [384/1280 (30%)]\tLoss: 0.490428\n",
      "Train Epoch: 10 [576/1280 (45%)]\tLoss: 0.595915\n",
      "Train Epoch: 10 [768/1280 (60%)]\tLoss: 0.477065\n",
      "Train Epoch: 10 [960/1280 (75%)]\tLoss: 0.535931\n",
      "Train Epoch: 10 [1152/1280 (90%)]\tLoss: 0.338443\n",
      "\n",
      "Test set: Average loss: 0.6369, Accuracy: 1014/1280 (79%)\n",
      "\n",
      "CPU times: user 10.9 s, sys: 1.84 s, total: 12.7 s\n",
      "Wall time: 13.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, less_train_dataloader, optimizer, epoch, workers)\n",
    "    test(args, model, device, less_test_dataloader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:worker1 #objects:35>\n"
     ]
    }
   ],
   "source": [
    "print(workers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:worker1 #objects:180>\n",
      "None\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Net' object has no attribute 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-212-293ccfed3689>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mmodel_ondevice\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maggregater\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    574\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 576\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Net' object has no attribute 'weight'"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "model_list = []\n",
    "model_ondevice = []\n",
    "aggregater = sy.VirtualWorker(hook, id=\"aggregater\")\n",
    "\n",
    "for batch_idx, (data, target) in enumerate(less_train_dataloader):\n",
    "    data_on_device = data.send(workers[batch_idx])\n",
    "    if batch_idx<3:\n",
    "        model_ondevice.append(model.copy().send(workers[batch_idx]))\n",
    "        print(model_ondevice[batch_idx].location)\n",
    "        \n",
    "        pre = model_ondevice[batch_idx](data_on_device)\n",
    "        model_ondevice[batch_idx].move(aggregater)\n",
    "        print(model.location)\n",
    "        print(model.weight.data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-215-fa2944c89d62>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_list\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    print(model_list[0].parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7fb2982e6e08>"
      ]
     },
     "execution_count": 221,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01297603,  0.02842132,  0.01014335, ..., -0.00745563,\n",
       "        -0.02407177, -0.00225234],\n",
       "       [ 0.03080916,  0.01323886,  0.03059926, ..., -0.01799139,\n",
       "        -0.0265331 ,  0.01559756],\n",
       "       [ 0.01701068,  0.00417169,  0.0316051 , ...,  0.03077709,\n",
       "        -0.02863705,  0.01833366],\n",
       "       ...,\n",
       "       [-0.00771326, -0.01363829,  0.03800831, ..., -0.04430421,\n",
       "        -0.03094019, -0.01628821],\n",
       "       [-0.0320928 ,  0.03452658,  0.01318498, ..., -0.0185255 ,\n",
       "         0.04329454,  0.01331757],\n",
       "       [ 0.02591962, -0.00609975,  0.00413742, ...,  0.02711744,\n",
       "        -0.00503955, -0.03584282]], dtype=float32)"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc2.weight.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
