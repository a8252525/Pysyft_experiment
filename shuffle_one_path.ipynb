{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "n_train_items = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "\n",
    "workers = connect_to_workers(n_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = 20\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 3\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ]))\n",
    "#     .federate(workers), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "less_train_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "less_test_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy \n",
    "# #mnist_dataset.__getitem__(2)[1]\n",
    "# a = (mnist_dataset.__getitem__(0)[0]).numpy()\n",
    "# a.dtype = 'uint8'\n",
    "# print(a)\n",
    "# Image.fromarray(a[0], mode= 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, less_train_dataloader, optimizer, epoch, workers):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(less_train_dataloader): # <-- now it is a distributed dataset\n",
    "        model.send(workers[batch_idx%len(workers)]) # <-- NEW: send the model to the right location\n",
    "        \n",
    "        data_on_worker = data.send(workers[batch_idx%len(workers)])\n",
    "        target_on_worker = target.send(workers[batch_idx%len(workers)])\n",
    "        \n",
    "        data_on_worker, target_on_worker = data_on_worker.to(device), target_on_worker.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_on_worker)\n",
    "        loss = F.nll_loss(output, target_on_worker)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(less_train_dataloader) * args.batch_size,\n",
    "                100. * batch_idx / len(less_train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader*args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader* args.batch_size),\n",
    "        100. * correct / (len(test_loader)*args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1280 (0%)]\tLoss: 2.309372\n",
      "Train Epoch: 1 [192/1280 (15%)]\tLoss: 2.293519\n",
      "Train Epoch: 1 [384/1280 (30%)]\tLoss: 2.280491\n",
      "Train Epoch: 1 [576/1280 (45%)]\tLoss: 2.270094\n",
      "Train Epoch: 1 [768/1280 (60%)]\tLoss: 2.257701\n",
      "Train Epoch: 1 [960/1280 (75%)]\tLoss: 2.254286\n",
      "Train Epoch: 1 [1152/1280 (90%)]\tLoss: 2.236799\n",
      "\n",
      "Test set: Average loss: 2.2361, Accuracy: 420/1280 (33%)\n",
      "\n",
      "Train Epoch: 2 [0/1280 (0%)]\tLoss: 2.233995\n",
      "Train Epoch: 2 [192/1280 (15%)]\tLoss: 2.214956\n",
      "Train Epoch: 2 [384/1280 (30%)]\tLoss: 2.204161\n",
      "Train Epoch: 2 [576/1280 (45%)]\tLoss: 2.200816\n",
      "Train Epoch: 2 [768/1280 (60%)]\tLoss: 2.175603\n",
      "Train Epoch: 2 [960/1280 (75%)]\tLoss: 2.160535\n",
      "Train Epoch: 2 [1152/1280 (90%)]\tLoss: 2.128215\n",
      "\n",
      "Test set: Average loss: 2.1525, Accuracy: 639/1280 (50%)\n",
      "\n",
      "Train Epoch: 3 [0/1280 (0%)]\tLoss: 2.140441\n",
      "Train Epoch: 3 [192/1280 (15%)]\tLoss: 2.101158\n",
      "Train Epoch: 3 [384/1280 (30%)]\tLoss: 2.094704\n",
      "Train Epoch: 3 [576/1280 (45%)]\tLoss: 2.096720\n",
      "Train Epoch: 3 [768/1280 (60%)]\tLoss: 2.046999\n",
      "Train Epoch: 3 [960/1280 (75%)]\tLoss: 2.013355\n",
      "Train Epoch: 3 [1152/1280 (90%)]\tLoss: 1.956735\n",
      "\n",
      "Test set: Average loss: 2.0097, Accuracy: 716/1280 (56%)\n",
      "\n",
      "Train Epoch: 4 [0/1280 (0%)]\tLoss: 1.982185\n",
      "Train Epoch: 4 [192/1280 (15%)]\tLoss: 1.895870\n",
      "Train Epoch: 4 [384/1280 (30%)]\tLoss: 1.898487\n",
      "Train Epoch: 4 [576/1280 (45%)]\tLoss: 1.904139\n",
      "Train Epoch: 4 [768/1280 (60%)]\tLoss: 1.806218\n",
      "Train Epoch: 4 [960/1280 (75%)]\tLoss: 1.742610\n",
      "Train Epoch: 4 [1152/1280 (90%)]\tLoss: 1.644675\n",
      "\n",
      "Test set: Average loss: 1.7446, Accuracy: 785/1280 (61%)\n",
      "\n",
      "Train Epoch: 5 [0/1280 (0%)]\tLoss: 1.697310\n",
      "Train Epoch: 5 [192/1280 (15%)]\tLoss: 1.529578\n",
      "Train Epoch: 5 [384/1280 (30%)]\tLoss: 1.552186\n",
      "Train Epoch: 5 [576/1280 (45%)]\tLoss: 1.576328\n",
      "Train Epoch: 5 [768/1280 (60%)]\tLoss: 1.406017\n",
      "Train Epoch: 5 [960/1280 (75%)]\tLoss: 1.330630\n",
      "Train Epoch: 5 [1152/1280 (90%)]\tLoss: 1.193089\n",
      "\n",
      "Test set: Average loss: 1.3623, Accuracy: 882/1280 (69%)\n",
      "\n",
      "Train Epoch: 6 [0/1280 (0%)]\tLoss: 1.298280\n",
      "Train Epoch: 6 [192/1280 (15%)]\tLoss: 1.066750\n",
      "Train Epoch: 6 [384/1280 (30%)]\tLoss: 1.124586\n",
      "Train Epoch: 6 [576/1280 (45%)]\tLoss: 1.203436\n",
      "Train Epoch: 6 [768/1280 (60%)]\tLoss: 0.991684\n",
      "Train Epoch: 6 [960/1280 (75%)]\tLoss: 0.962850\n",
      "Train Epoch: 6 [1152/1280 (90%)]\tLoss: 0.811011\n",
      "\n",
      "Test set: Average loss: 1.0462, Accuracy: 943/1280 (74%)\n",
      "\n",
      "Train Epoch: 7 [0/1280 (0%)]\tLoss: 0.977854\n",
      "Train Epoch: 7 [192/1280 (15%)]\tLoss: 0.738923\n",
      "Train Epoch: 7 [384/1280 (30%)]\tLoss: 0.806187\n",
      "Train Epoch: 7 [576/1280 (45%)]\tLoss: 0.942830\n",
      "Train Epoch: 7 [768/1280 (60%)]\tLoss: 0.751772\n",
      "Train Epoch: 7 [960/1280 (75%)]\tLoss: 0.752732\n",
      "Train Epoch: 7 [1152/1280 (90%)]\tLoss: 0.592902\n",
      "\n",
      "Test set: Average loss: 0.8626, Accuracy: 966/1280 (75%)\n",
      "\n",
      "Train Epoch: 8 [0/1280 (0%)]\tLoss: 0.780668\n",
      "Train Epoch: 8 [192/1280 (15%)]\tLoss: 0.571942\n",
      "Train Epoch: 8 [384/1280 (30%)]\tLoss: 0.621927\n",
      "Train Epoch: 8 [576/1280 (45%)]\tLoss: 0.790383\n",
      "Train Epoch: 8 [768/1280 (60%)]\tLoss: 0.631434\n",
      "Train Epoch: 8 [960/1280 (75%)]\tLoss: 0.639566\n",
      "Train Epoch: 8 [1152/1280 (90%)]\tLoss: 0.470231\n",
      "\n",
      "Test set: Average loss: 0.7616, Accuracy: 979/1280 (76%)\n",
      "\n",
      "Train Epoch: 9 [0/1280 (0%)]\tLoss: 0.666030\n",
      "Train Epoch: 9 [192/1280 (15%)]\tLoss: 0.496714\n",
      "Train Epoch: 9 [384/1280 (30%)]\tLoss: 0.515304\n",
      "Train Epoch: 9 [576/1280 (45%)]\tLoss: 0.688410\n",
      "Train Epoch: 9 [768/1280 (60%)]\tLoss: 0.548399\n",
      "Train Epoch: 9 [960/1280 (75%)]\tLoss: 0.569322\n",
      "Train Epoch: 9 [1152/1280 (90%)]\tLoss: 0.393275\n",
      "\n",
      "Test set: Average loss: 0.6986, Accuracy: 993/1280 (78%)\n",
      "\n",
      "Train Epoch: 10 [0/1280 (0%)]\tLoss: 0.596711\n",
      "Train Epoch: 10 [192/1280 (15%)]\tLoss: 0.459929\n",
      "Train Epoch: 10 [384/1280 (30%)]\tLoss: 0.447134\n",
      "Train Epoch: 10 [576/1280 (45%)]\tLoss: 0.609752\n",
      "Train Epoch: 10 [768/1280 (60%)]\tLoss: 0.485316\n",
      "Train Epoch: 10 [960/1280 (75%)]\tLoss: 0.515607\n",
      "Train Epoch: 10 [1152/1280 (90%)]\tLoss: 0.338916\n",
      "\n",
      "Test set: Average loss: 0.6486, Accuracy: 1011/1280 (79%)\n",
      "\n",
      "Train Epoch: 11 [0/1280 (0%)]\tLoss: 0.546470\n",
      "Train Epoch: 11 [192/1280 (15%)]\tLoss: 0.436658\n",
      "Train Epoch: 11 [384/1280 (30%)]\tLoss: 0.401146\n",
      "Train Epoch: 11 [576/1280 (45%)]\tLoss: 0.549051\n",
      "Train Epoch: 11 [768/1280 (60%)]\tLoss: 0.440664\n",
      "Train Epoch: 11 [960/1280 (75%)]\tLoss: 0.469203\n",
      "Train Epoch: 11 [1152/1280 (90%)]\tLoss: 0.297470\n",
      "\n",
      "Test set: Average loss: 0.6049, Accuracy: 1026/1280 (80%)\n",
      "\n",
      "Train Epoch: 12 [0/1280 (0%)]\tLoss: 0.506287\n",
      "Train Epoch: 12 [192/1280 (15%)]\tLoss: 0.418675\n",
      "Train Epoch: 12 [384/1280 (30%)]\tLoss: 0.367901\n",
      "Train Epoch: 12 [576/1280 (45%)]\tLoss: 0.501286\n",
      "Train Epoch: 12 [768/1280 (60%)]\tLoss: 0.411317\n",
      "Train Epoch: 12 [960/1280 (75%)]\tLoss: 0.429774\n",
      "Train Epoch: 12 [1152/1280 (90%)]\tLoss: 0.263878\n",
      "\n",
      "Test set: Average loss: 0.5682, Accuracy: 1046/1280 (82%)\n",
      "\n",
      "Train Epoch: 13 [0/1280 (0%)]\tLoss: 0.474550\n",
      "Train Epoch: 13 [192/1280 (15%)]\tLoss: 0.402550\n",
      "Train Epoch: 13 [384/1280 (30%)]\tLoss: 0.342617\n",
      "Train Epoch: 13 [576/1280 (45%)]\tLoss: 0.462775\n",
      "Train Epoch: 13 [768/1280 (60%)]\tLoss: 0.389608\n",
      "Train Epoch: 13 [960/1280 (75%)]\tLoss: 0.396750\n",
      "Train Epoch: 13 [1152/1280 (90%)]\tLoss: 0.236208\n",
      "\n",
      "Test set: Average loss: 0.5379, Accuracy: 1061/1280 (83%)\n",
      "\n",
      "Train Epoch: 14 [0/1280 (0%)]\tLoss: 0.450604\n",
      "Train Epoch: 14 [192/1280 (15%)]\tLoss: 0.387221\n",
      "Train Epoch: 14 [384/1280 (30%)]\tLoss: 0.323070\n",
      "Train Epoch: 14 [576/1280 (45%)]\tLoss: 0.430723\n",
      "Train Epoch: 14 [768/1280 (60%)]\tLoss: 0.371195\n",
      "Train Epoch: 14 [960/1280 (75%)]\tLoss: 0.368335\n",
      "Train Epoch: 14 [1152/1280 (90%)]\tLoss: 0.214592\n",
      "\n",
      "Test set: Average loss: 0.5149, Accuracy: 1072/1280 (84%)\n",
      "\n",
      "Train Epoch: 15 [0/1280 (0%)]\tLoss: 0.432267\n",
      "Train Epoch: 15 [192/1280 (15%)]\tLoss: 0.372440\n",
      "Train Epoch: 15 [384/1280 (30%)]\tLoss: 0.307233\n",
      "Train Epoch: 15 [576/1280 (45%)]\tLoss: 0.403556\n",
      "Train Epoch: 15 [768/1280 (60%)]\tLoss: 0.354162\n",
      "Train Epoch: 15 [960/1280 (75%)]\tLoss: 0.343507\n",
      "Train Epoch: 15 [1152/1280 (90%)]\tLoss: 0.197373\n",
      "\n",
      "Test set: Average loss: 0.4968, Accuracy: 1080/1280 (84%)\n",
      "\n",
      "Train Epoch: 16 [0/1280 (0%)]\tLoss: 0.418091\n",
      "Train Epoch: 16 [192/1280 (15%)]\tLoss: 0.358956\n",
      "Train Epoch: 16 [384/1280 (30%)]\tLoss: 0.293434\n",
      "Train Epoch: 16 [576/1280 (45%)]\tLoss: 0.381139\n",
      "Train Epoch: 16 [768/1280 (60%)]\tLoss: 0.337924\n",
      "Train Epoch: 16 [960/1280 (75%)]\tLoss: 0.321479\n",
      "Train Epoch: 16 [1152/1280 (90%)]\tLoss: 0.183471\n",
      "\n",
      "Test set: Average loss: 0.4812, Accuracy: 1088/1280 (85%)\n",
      "\n",
      "Train Epoch: 17 [0/1280 (0%)]\tLoss: 0.404871\n",
      "Train Epoch: 17 [192/1280 (15%)]\tLoss: 0.345767\n",
      "Train Epoch: 17 [384/1280 (30%)]\tLoss: 0.281457\n",
      "Train Epoch: 17 [576/1280 (45%)]\tLoss: 0.360567\n",
      "Train Epoch: 17 [768/1280 (60%)]\tLoss: 0.321720\n",
      "Train Epoch: 17 [960/1280 (75%)]\tLoss: 0.302032\n",
      "Train Epoch: 17 [1152/1280 (90%)]\tLoss: 0.172077\n",
      "\n",
      "Test set: Average loss: 0.4678, Accuracy: 1093/1280 (85%)\n",
      "\n",
      "Train Epoch: 18 [0/1280 (0%)]\tLoss: 0.393824\n",
      "Train Epoch: 18 [192/1280 (15%)]\tLoss: 0.333248\n",
      "Train Epoch: 18 [384/1280 (30%)]\tLoss: 0.270623\n",
      "Train Epoch: 18 [576/1280 (45%)]\tLoss: 0.342262\n",
      "Train Epoch: 18 [768/1280 (60%)]\tLoss: 0.307080\n",
      "Train Epoch: 18 [960/1280 (75%)]\tLoss: 0.284603\n",
      "Train Epoch: 18 [1152/1280 (90%)]\tLoss: 0.162225\n",
      "\n",
      "Test set: Average loss: 0.4561, Accuracy: 1099/1280 (86%)\n",
      "\n",
      "Train Epoch: 19 [0/1280 (0%)]\tLoss: 0.384288\n",
      "Train Epoch: 19 [192/1280 (15%)]\tLoss: 0.321226\n",
      "Train Epoch: 19 [384/1280 (30%)]\tLoss: 0.260916\n",
      "Train Epoch: 19 [576/1280 (45%)]\tLoss: 0.325566\n",
      "Train Epoch: 19 [768/1280 (60%)]\tLoss: 0.293076\n",
      "Train Epoch: 19 [960/1280 (75%)]\tLoss: 0.269177\n",
      "Train Epoch: 19 [1152/1280 (90%)]\tLoss: 0.153694\n",
      "\n",
      "Test set: Average loss: 0.4454, Accuracy: 1107/1280 (86%)\n",
      "\n",
      "Train Epoch: 20 [0/1280 (0%)]\tLoss: 0.373866\n",
      "Train Epoch: 20 [192/1280 (15%)]\tLoss: 0.309614\n",
      "Train Epoch: 20 [384/1280 (30%)]\tLoss: 0.251822\n",
      "Train Epoch: 20 [576/1280 (45%)]\tLoss: 0.310928\n",
      "Train Epoch: 20 [768/1280 (60%)]\tLoss: 0.279279\n",
      "Train Epoch: 20 [960/1280 (75%)]\tLoss: 0.255514\n",
      "Train Epoch: 20 [1152/1280 (90%)]\tLoss: 0.145650\n",
      "\n",
      "Test set: Average loss: 0.4351, Accuracy: 1111/1280 (87%)\n",
      "\n",
      "CPU times: user 18.1 s, sys: 0 ns, total: 18.1 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, less_train_dataloader, optimizer, epoch, workers)\n",
    "    test(args, model, device, less_test_dataloader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:worker1 #objects:6>\n"
     ]
    }
   ],
   "source": [
    "print(workers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor | me:60407973905 -> worker1:78163135291]\n",
      "(Wrapper)>[PointerTensor | me:74050153873 -> worker1:11814332395]\n",
      "(Wrapper)>[PointerTensor | me:93651540692 -> worker1:98042234477]\n",
      "(Wrapper)>[PointerTensor | me:1792853962 -> worker1:10207084528]\n",
      "(Wrapper)>[PointerTensor | me:84209531306 -> worker1:30075516339]\n",
      "(Wrapper)>[PointerTensor | me:4457758866 -> worker1:81018670476]\n",
      "(Wrapper)>[PointerTensor | me:37733711282 -> worker1:94469615650]\n",
      "(Wrapper)>[PointerTensor | me:41956409820 -> worker1:31235277075]\n",
      "(Wrapper)>[PointerTensor | me:894364867 -> worker1:14226006381]\n",
      "(Wrapper)>[PointerTensor | me:38101863086 -> worker1:87171777304]\n",
      "(Wrapper)>[PointerTensor | me:95239140209 -> worker1:51638303354]\n",
      "(Wrapper)>[PointerTensor | me:35612061462 -> worker1:68752432269]\n",
      "(Wrapper)>[PointerTensor | me:64152108992 -> worker1:31182305053]\n",
      "(Wrapper)>[PointerTensor | me:51875992271 -> worker1:65242662746]\n",
      "(Wrapper)>[PointerTensor | me:86903732979 -> worker1:47402955970]\n",
      "(Wrapper)>[PointerTensor | me:74349443719 -> worker1:62935393057]\n",
      "(Wrapper)>[PointerTensor | me:24641961433 -> worker1:27217973564]\n",
      "(Wrapper)>[PointerTensor | me:66355971699 -> worker1:79600686106]\n",
      "(Wrapper)>[PointerTensor | me:35692158590 -> worker1:84083579481]\n",
      "(Wrapper)>[PointerTensor | me:75134439842 -> worker1:86741308451]\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "for batch_idx, (data, target) in enumerate(less_train_dataloader):\n",
    "    data = data.send(workers[0])\n",
    "    print(data)\n",
    "#     if batch_idx<3:\n",
    "#         model.send(workers[0])\n",
    "#         print(data.size())\n",
    "        \n",
    "#         pre = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers[0]._objects[50490937571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
