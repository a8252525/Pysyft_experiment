{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "n_train_items = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import syft as sy  # <-- NEW: import the Pysyft library\n",
    "hook = sy.TorchHook(torch)  # <-- NEW: hook PyTorch ie add extra functionalities to support Federated Learning\n",
    "# simulation functions\n",
    "def connect_to_workers(n_workers):\n",
    "    return [\n",
    "        sy.VirtualWorker(hook, id=f\"worker{i+1}\")\n",
    "        for i in range(n_workers)\n",
    "    ]\n",
    "\n",
    "\n",
    "workers = connect_to_workers(n_workers=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = 20\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 3\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# federated_train_loader = sy.FederatedDataLoader( # <-- this is now a FederatedDataLoader \n",
    "#     datasets.MNIST('../data', train=True, download=True,\n",
    "#                    transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ]))\n",
    "#     .federate(workers), # <-- NEW: we distribute the dataset across all the workers, it's now a FederatedDataset\n",
    "#     batch_size=args.batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "# test_loader = torch.utils.data.DataLoader(\n",
    "#     datasets.MNIST('../data', train=False, transform=transforms.Compose([\n",
    "#                        transforms.ToTensor(),\n",
    "#                        transforms.Normalize((0.1307,), (0.3081,))\n",
    "#                    ])),\n",
    "#     batch_size=args.test_batch_size, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "less_train_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "less_test_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(test_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from PIL import Image\n",
    "# import numpy \n",
    "# #mnist_dataset.__getitem__(2)[1]\n",
    "# a = (mnist_dataset.__getitem__(0)[0]).numpy()\n",
    "# a.dtype = 'uint8'\n",
    "# print(a)\n",
    "# Image.fromarray(a[0], mode= 'P')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, model, device, less_train_dataloader, optimizer, epoch, workers):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(less_train_dataloader): # <-- now it is a distributed dataset\n",
    "        model.send(workers[batch_idx%len(workers)]) # <-- NEW: send the model to the right location\n",
    "        \n",
    "        data_on_worker = data.send(workers[batch_idx%len(workers)])\n",
    "        target_on_worker = target.send(workers[batch_idx%len(workers)])\n",
    "        \n",
    "        data_on_worker, target_on_worker = data_on_worker.to(device), target_on_worker.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data_on_worker)\n",
    "        loss = F.nll_loss(output, target_on_worker)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.get() # <-- NEW: get the model back\n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            loss = loss.get() # <-- NEW: get the loss back\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(less_train_dataloader) * args.batch_size,\n",
    "                100. * batch_idx / len(less_train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader*args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader* args.batch_size),\n",
    "        100. * correct / (len(test_loader)*args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1280 (0%)]\tLoss: 2.288257\n",
      "Train Epoch: 1 [192/1280 (15%)]\tLoss: 2.282528\n",
      "Train Epoch: 1 [384/1280 (30%)]\tLoss: 2.288453\n",
      "Train Epoch: 1 [576/1280 (45%)]\tLoss: 2.275638\n",
      "Train Epoch: 1 [768/1280 (60%)]\tLoss: 2.265264\n",
      "Train Epoch: 1 [960/1280 (75%)]\tLoss: 2.232260\n",
      "Train Epoch: 1 [1152/1280 (90%)]\tLoss: 2.213333\n",
      "\n",
      "Test set: Average loss: 2.2409, Accuracy: 372/1280 (29%)\n",
      "\n",
      "Train Epoch: 2 [0/1280 (0%)]\tLoss: 2.221643\n",
      "Train Epoch: 2 [192/1280 (15%)]\tLoss: 2.202060\n",
      "Train Epoch: 2 [384/1280 (30%)]\tLoss: 2.210339\n",
      "Train Epoch: 2 [576/1280 (45%)]\tLoss: 2.205838\n",
      "Train Epoch: 2 [768/1280 (60%)]\tLoss: 2.178711\n",
      "Train Epoch: 2 [960/1280 (75%)]\tLoss: 2.142342\n",
      "Train Epoch: 2 [1152/1280 (90%)]\tLoss: 2.112935\n",
      "\n",
      "Test set: Average loss: 2.1541, Accuracy: 481/1280 (38%)\n",
      "\n",
      "Train Epoch: 3 [0/1280 (0%)]\tLoss: 2.125770\n",
      "Train Epoch: 3 [192/1280 (15%)]\tLoss: 2.078243\n",
      "Train Epoch: 3 [384/1280 (30%)]\tLoss: 2.091996\n",
      "Train Epoch: 3 [576/1280 (45%)]\tLoss: 2.094636\n",
      "Train Epoch: 3 [768/1280 (60%)]\tLoss: 2.040481\n",
      "Train Epoch: 3 [960/1280 (75%)]\tLoss: 1.989579\n",
      "Train Epoch: 3 [1152/1280 (90%)]\tLoss: 1.943518\n",
      "\n",
      "Test set: Average loss: 2.0013, Accuracy: 615/1280 (48%)\n",
      "\n",
      "Train Epoch: 4 [0/1280 (0%)]\tLoss: 1.954342\n",
      "Train Epoch: 4 [192/1280 (15%)]\tLoss: 1.850341\n",
      "Train Epoch: 4 [384/1280 (30%)]\tLoss: 1.882864\n",
      "Train Epoch: 4 [576/1280 (45%)]\tLoss: 1.891948\n",
      "Train Epoch: 4 [768/1280 (60%)]\tLoss: 1.783902\n",
      "Train Epoch: 4 [960/1280 (75%)]\tLoss: 1.704982\n",
      "Train Epoch: 4 [1152/1280 (90%)]\tLoss: 1.633133\n",
      "\n",
      "Test set: Average loss: 1.7253, Accuracy: 707/1280 (55%)\n",
      "\n",
      "Train Epoch: 5 [0/1280 (0%)]\tLoss: 1.645728\n",
      "Train Epoch: 5 [192/1280 (15%)]\tLoss: 1.462418\n",
      "Train Epoch: 5 [384/1280 (30%)]\tLoss: 1.532112\n",
      "Train Epoch: 5 [576/1280 (45%)]\tLoss: 1.561883\n",
      "Train Epoch: 5 [768/1280 (60%)]\tLoss: 1.371063\n",
      "Train Epoch: 5 [960/1280 (75%)]\tLoss: 1.281582\n",
      "Train Epoch: 5 [1152/1280 (90%)]\tLoss: 1.201314\n",
      "\n",
      "Test set: Average loss: 1.3563, Accuracy: 806/1280 (63%)\n",
      "\n",
      "Train Epoch: 6 [0/1280 (0%)]\tLoss: 1.248590\n",
      "Train Epoch: 6 [192/1280 (15%)]\tLoss: 1.026917\n",
      "Train Epoch: 6 [384/1280 (30%)]\tLoss: 1.136793\n",
      "Train Epoch: 6 [576/1280 (45%)]\tLoss: 1.212382\n",
      "Train Epoch: 6 [768/1280 (60%)]\tLoss: 0.980481\n",
      "Train Epoch: 6 [960/1280 (75%)]\tLoss: 0.920914\n",
      "Train Epoch: 6 [1152/1280 (90%)]\tLoss: 0.840357\n",
      "\n",
      "Test set: Average loss: 1.0623, Accuracy: 901/1280 (70%)\n",
      "\n",
      "Train Epoch: 7 [0/1280 (0%)]\tLoss: 0.961885\n",
      "Train Epoch: 7 [192/1280 (15%)]\tLoss: 0.730996\n",
      "Train Epoch: 7 [384/1280 (30%)]\tLoss: 0.855852\n",
      "Train Epoch: 7 [576/1280 (45%)]\tLoss: 0.972377\n",
      "Train Epoch: 7 [768/1280 (60%)]\tLoss: 0.782535\n",
      "Train Epoch: 7 [960/1280 (75%)]\tLoss: 0.724037\n",
      "Train Epoch: 7 [1152/1280 (90%)]\tLoss: 0.621657\n",
      "\n",
      "Test set: Average loss: 0.8874, Accuracy: 940/1280 (73%)\n",
      "\n",
      "Train Epoch: 8 [0/1280 (0%)]\tLoss: 0.783402\n",
      "Train Epoch: 8 [192/1280 (15%)]\tLoss: 0.575321\n",
      "Train Epoch: 8 [384/1280 (30%)]\tLoss: 0.692704\n",
      "Train Epoch: 8 [576/1280 (45%)]\tLoss: 0.828680\n",
      "Train Epoch: 8 [768/1280 (60%)]\tLoss: 0.697834\n",
      "Train Epoch: 8 [960/1280 (75%)]\tLoss: 0.629525\n",
      "Train Epoch: 8 [1152/1280 (90%)]\tLoss: 0.492884\n",
      "\n",
      "Test set: Average loss: 0.7978, Accuracy: 947/1280 (74%)\n",
      "\n",
      "Train Epoch: 9 [0/1280 (0%)]\tLoss: 0.675582\n",
      "Train Epoch: 9 [192/1280 (15%)]\tLoss: 0.499467\n",
      "Train Epoch: 9 [384/1280 (30%)]\tLoss: 0.590145\n",
      "Train Epoch: 9 [576/1280 (45%)]\tLoss: 0.730256\n",
      "Train Epoch: 9 [768/1280 (60%)]\tLoss: 0.622174\n",
      "Train Epoch: 9 [960/1280 (75%)]\tLoss: 0.572973\n",
      "Train Epoch: 9 [1152/1280 (90%)]\tLoss: 0.411335\n",
      "\n",
      "Test set: Average loss: 0.7504, Accuracy: 962/1280 (75%)\n",
      "\n",
      "Train Epoch: 10 [0/1280 (0%)]\tLoss: 0.612620\n",
      "Train Epoch: 10 [192/1280 (15%)]\tLoss: 0.458687\n",
      "Train Epoch: 10 [384/1280 (30%)]\tLoss: 0.519332\n",
      "Train Epoch: 10 [576/1280 (45%)]\tLoss: 0.654536\n",
      "Train Epoch: 10 [768/1280 (60%)]\tLoss: 0.541620\n",
      "Train Epoch: 10 [960/1280 (75%)]\tLoss: 0.531214\n",
      "Train Epoch: 10 [1152/1280 (90%)]\tLoss: 0.355907\n",
      "\n",
      "Test set: Average loss: 0.7129, Accuracy: 972/1280 (76%)\n",
      "\n",
      "Train Epoch: 11 [0/1280 (0%)]\tLoss: 0.567923\n",
      "Train Epoch: 11 [192/1280 (15%)]\tLoss: 0.431455\n",
      "Train Epoch: 11 [384/1280 (30%)]\tLoss: 0.468375\n",
      "Train Epoch: 11 [576/1280 (45%)]\tLoss: 0.593057\n",
      "Train Epoch: 11 [768/1280 (60%)]\tLoss: 0.474809\n",
      "Train Epoch: 11 [960/1280 (75%)]\tLoss: 0.494136\n",
      "Train Epoch: 11 [1152/1280 (90%)]\tLoss: 0.315183\n",
      "\n",
      "Test set: Average loss: 0.6783, Accuracy: 979/1280 (76%)\n",
      "\n",
      "Train Epoch: 12 [0/1280 (0%)]\tLoss: 0.532355\n",
      "Train Epoch: 12 [192/1280 (15%)]\tLoss: 0.409991\n",
      "Train Epoch: 12 [384/1280 (30%)]\tLoss: 0.429056\n",
      "Train Epoch: 12 [576/1280 (45%)]\tLoss: 0.543471\n",
      "Train Epoch: 12 [768/1280 (60%)]\tLoss: 0.427502\n",
      "Train Epoch: 12 [960/1280 (75%)]\tLoss: 0.459843\n",
      "Train Epoch: 12 [1152/1280 (90%)]\tLoss: 0.282632\n",
      "\n",
      "Test set: Average loss: 0.6435, Accuracy: 984/1280 (77%)\n",
      "\n",
      "Train Epoch: 13 [0/1280 (0%)]\tLoss: 0.501338\n",
      "Train Epoch: 13 [192/1280 (15%)]\tLoss: 0.390688\n",
      "Train Epoch: 13 [384/1280 (30%)]\tLoss: 0.397417\n",
      "Train Epoch: 13 [576/1280 (45%)]\tLoss: 0.500968\n",
      "Train Epoch: 13 [768/1280 (60%)]\tLoss: 0.394786\n",
      "Train Epoch: 13 [960/1280 (75%)]\tLoss: 0.429659\n",
      "Train Epoch: 13 [1152/1280 (90%)]\tLoss: 0.256196\n",
      "\n",
      "Test set: Average loss: 0.6124, Accuracy: 999/1280 (78%)\n",
      "\n",
      "Train Epoch: 14 [0/1280 (0%)]\tLoss: 0.476939\n",
      "Train Epoch: 14 [192/1280 (15%)]\tLoss: 0.373928\n",
      "Train Epoch: 14 [384/1280 (30%)]\tLoss: 0.371636\n",
      "Train Epoch: 14 [576/1280 (45%)]\tLoss: 0.465151\n",
      "Train Epoch: 14 [768/1280 (60%)]\tLoss: 0.374169\n",
      "Train Epoch: 14 [960/1280 (75%)]\tLoss: 0.402157\n",
      "Train Epoch: 14 [1152/1280 (90%)]\tLoss: 0.233768\n",
      "\n",
      "Test set: Average loss: 0.5811, Accuracy: 1005/1280 (79%)\n",
      "\n",
      "Train Epoch: 15 [0/1280 (0%)]\tLoss: 0.454588\n",
      "Train Epoch: 15 [192/1280 (15%)]\tLoss: 0.357638\n",
      "Train Epoch: 15 [384/1280 (30%)]\tLoss: 0.350111\n",
      "Train Epoch: 15 [576/1280 (45%)]\tLoss: 0.433609\n",
      "Train Epoch: 15 [768/1280 (60%)]\tLoss: 0.358802\n",
      "Train Epoch: 15 [960/1280 (75%)]\tLoss: 0.378240\n",
      "Train Epoch: 15 [1152/1280 (90%)]\tLoss: 0.215463\n",
      "\n",
      "Test set: Average loss: 0.5544, Accuracy: 1021/1280 (80%)\n",
      "\n",
      "Train Epoch: 16 [0/1280 (0%)]\tLoss: 0.436607\n",
      "Train Epoch: 16 [192/1280 (15%)]\tLoss: 0.342315\n",
      "Train Epoch: 16 [384/1280 (30%)]\tLoss: 0.331536\n",
      "Train Epoch: 16 [576/1280 (45%)]\tLoss: 0.406048\n",
      "Train Epoch: 16 [768/1280 (60%)]\tLoss: 0.347559\n",
      "Train Epoch: 16 [960/1280 (75%)]\tLoss: 0.356003\n",
      "Train Epoch: 16 [1152/1280 (90%)]\tLoss: 0.200171\n",
      "\n",
      "Test set: Average loss: 0.5301, Accuracy: 1042/1280 (81%)\n",
      "\n",
      "Train Epoch: 17 [0/1280 (0%)]\tLoss: 0.420840\n",
      "Train Epoch: 17 [192/1280 (15%)]\tLoss: 0.327819\n",
      "Train Epoch: 17 [384/1280 (30%)]\tLoss: 0.315184\n",
      "Train Epoch: 17 [576/1280 (45%)]\tLoss: 0.381808\n",
      "Train Epoch: 17 [768/1280 (60%)]\tLoss: 0.336901\n",
      "Train Epoch: 17 [960/1280 (75%)]\tLoss: 0.334872\n",
      "Train Epoch: 17 [1152/1280 (90%)]\tLoss: 0.187157\n",
      "\n",
      "Test set: Average loss: 0.5099, Accuracy: 1050/1280 (82%)\n",
      "\n",
      "Train Epoch: 18 [0/1280 (0%)]\tLoss: 0.408363\n",
      "Train Epoch: 18 [192/1280 (15%)]\tLoss: 0.314233\n",
      "Train Epoch: 18 [384/1280 (30%)]\tLoss: 0.300064\n",
      "Train Epoch: 18 [576/1280 (45%)]\tLoss: 0.359644\n",
      "Train Epoch: 18 [768/1280 (60%)]\tLoss: 0.326447\n",
      "Train Epoch: 18 [960/1280 (75%)]\tLoss: 0.316647\n",
      "Train Epoch: 18 [1152/1280 (90%)]\tLoss: 0.175701\n",
      "\n",
      "Test set: Average loss: 0.4925, Accuracy: 1058/1280 (83%)\n",
      "\n",
      "Train Epoch: 19 [0/1280 (0%)]\tLoss: 0.396613\n",
      "Train Epoch: 19 [192/1280 (15%)]\tLoss: 0.301886\n",
      "Train Epoch: 19 [384/1280 (30%)]\tLoss: 0.286922\n",
      "Train Epoch: 19 [576/1280 (45%)]\tLoss: 0.339416\n",
      "Train Epoch: 19 [768/1280 (60%)]\tLoss: 0.315974\n",
      "Train Epoch: 19 [960/1280 (75%)]\tLoss: 0.300388\n",
      "Train Epoch: 19 [1152/1280 (90%)]\tLoss: 0.165855\n",
      "\n",
      "Test set: Average loss: 0.4773, Accuracy: 1078/1280 (84%)\n",
      "\n",
      "Train Epoch: 20 [0/1280 (0%)]\tLoss: 0.385051\n",
      "Train Epoch: 20 [192/1280 (15%)]\tLoss: 0.289912\n",
      "Train Epoch: 20 [384/1280 (30%)]\tLoss: 0.275283\n",
      "Train Epoch: 20 [576/1280 (45%)]\tLoss: 0.321349\n",
      "Train Epoch: 20 [768/1280 (60%)]\tLoss: 0.304169\n",
      "Train Epoch: 20 [960/1280 (75%)]\tLoss: 0.284950\n",
      "Train Epoch: 20 [1152/1280 (90%)]\tLoss: 0.156790\n",
      "\n",
      "Test set: Average loss: 0.4637, Accuracy: 1083/1280 (85%)\n",
      "\n",
      "CPU times: user 18 s, sys: 0 ns, total: 18 s\n",
      "Wall time: 18 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=args.lr) # TODO momentum is not supported at the moment\n",
    "\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, model, device, less_train_dataloader, optimizer, epoch, workers)\n",
    "    test(args, model, device, less_test_dataloader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<VirtualWorker id:worker1 #objects:6>\n"
     ]
    }
   ],
   "source": [
    "print(workers[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Wrapper)>[PointerTensor | me:60407973905 -> worker1:78163135291]\n",
      "(Wrapper)>[PointerTensor | me:74050153873 -> worker1:11814332395]\n",
      "(Wrapper)>[PointerTensor | me:93651540692 -> worker1:98042234477]\n",
      "(Wrapper)>[PointerTensor | me:1792853962 -> worker1:10207084528]\n",
      "(Wrapper)>[PointerTensor | me:84209531306 -> worker1:30075516339]\n",
      "(Wrapper)>[PointerTensor | me:4457758866 -> worker1:81018670476]\n",
      "(Wrapper)>[PointerTensor | me:37733711282 -> worker1:94469615650]\n",
      "(Wrapper)>[PointerTensor | me:41956409820 -> worker1:31235277075]\n",
      "(Wrapper)>[PointerTensor | me:894364867 -> worker1:14226006381]\n",
      "(Wrapper)>[PointerTensor | me:38101863086 -> worker1:87171777304]\n",
      "(Wrapper)>[PointerTensor | me:95239140209 -> worker1:51638303354]\n",
      "(Wrapper)>[PointerTensor | me:35612061462 -> worker1:68752432269]\n",
      "(Wrapper)>[PointerTensor | me:64152108992 -> worker1:31182305053]\n",
      "(Wrapper)>[PointerTensor | me:51875992271 -> worker1:65242662746]\n",
      "(Wrapper)>[PointerTensor | me:86903732979 -> worker1:47402955970]\n",
      "(Wrapper)>[PointerTensor | me:74349443719 -> worker1:62935393057]\n",
      "(Wrapper)>[PointerTensor | me:24641961433 -> worker1:27217973564]\n",
      "(Wrapper)>[PointerTensor | me:66355971699 -> worker1:79600686106]\n",
      "(Wrapper)>[PointerTensor | me:35692158590 -> worker1:84083579481]\n",
      "(Wrapper)>[PointerTensor | me:75134439842 -> worker1:86741308451]\n"
     ]
    }
   ],
   "source": [
    "model = Net()\n",
    "for batch_idx, (data, target) in enumerate(less_train_dataloader):\n",
    "    data = data.send(workers[0])\n",
    "    print(data)\n",
    "#     if batch_idx<3:\n",
    "#         model.send(workers[0])\n",
    "#         print(data.size())\n",
    "        \n",
    "#         pre = model(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        ...,\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]],\n",
       "\n",
       "\n",
       "        [[[-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          ...,\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242],\n",
       "          [-0.4242, -0.4242, -0.4242,  ..., -0.4242, -0.4242, -0.4242]]]])"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "workers[0]._objects[50490937571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
