{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use centrailzed training to compare with federated learning\n",
    "epochs = 20\n",
    "n_train_items = 1280"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 3\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "train_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "# test_dataloader = [\n",
    "#         ((data), (target))\n",
    "#         for i, (data, target) in enumerate(test_loader)\n",
    "#         if i < n_train_items / args.batch_size\n",
    "#     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataloader))\n",
    "#len(list(test_dataloader.__iter__())[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device, model, train_dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_dataloader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, device, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)*(args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader)* (args.batch_size),\n",
    "        100. * correct / (len(test_loader)*args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/1280 (0%)]\tLoss: 2.314400\n",
      "Train Epoch: 1 [192/1280 (15%)]\tLoss: 2.324195\n",
      "Train Epoch: 1 [384/1280 (30%)]\tLoss: 2.298956\n",
      "Train Epoch: 1 [576/1280 (45%)]\tLoss: 2.287949\n",
      "Train Epoch: 1 [768/1280 (60%)]\tLoss: 2.273747\n",
      "Train Epoch: 1 [960/1280 (75%)]\tLoss: 2.279161\n",
      "Train Epoch: 1 [1152/1280 (90%)]\tLoss: 2.265763\n",
      "\n",
      "Test set: Average loss: 2.2627, Accuracy: 2135/10048 (21%)\n",
      "\n",
      "Train Epoch: 2 [0/1280 (0%)]\tLoss: 2.269754\n",
      "Train Epoch: 2 [192/1280 (15%)]\tLoss: 2.276045\n",
      "Train Epoch: 2 [384/1280 (30%)]\tLoss: 2.257073\n",
      "Train Epoch: 2 [576/1280 (45%)]\tLoss: 2.252402\n",
      "Train Epoch: 2 [768/1280 (60%)]\tLoss: 2.235370\n",
      "Train Epoch: 2 [960/1280 (75%)]\tLoss: 2.230212\n",
      "Train Epoch: 2 [1152/1280 (90%)]\tLoss: 2.214319\n",
      "\n",
      "Test set: Average loss: 2.2169, Accuracy: 4117/10048 (41%)\n",
      "\n",
      "Train Epoch: 3 [0/1280 (0%)]\tLoss: 2.219200\n",
      "Train Epoch: 3 [192/1280 (15%)]\tLoss: 2.219878\n",
      "Train Epoch: 3 [384/1280 (30%)]\tLoss: 2.203995\n",
      "Train Epoch: 3 [576/1280 (45%)]\tLoss: 2.204735\n",
      "Train Epoch: 3 [768/1280 (60%)]\tLoss: 2.182621\n",
      "Train Epoch: 3 [960/1280 (75%)]\tLoss: 2.164164\n",
      "Train Epoch: 3 [1152/1280 (90%)]\tLoss: 2.141609\n",
      "\n",
      "Test set: Average loss: 2.1504, Accuracy: 4833/10048 (48%)\n",
      "\n",
      "Train Epoch: 4 [0/1280 (0%)]\tLoss: 2.148047\n",
      "Train Epoch: 4 [192/1280 (15%)]\tLoss: 2.137346\n",
      "Train Epoch: 4 [384/1280 (30%)]\tLoss: 2.124136\n",
      "Train Epoch: 4 [576/1280 (45%)]\tLoss: 2.128922\n",
      "Train Epoch: 4 [768/1280 (60%)]\tLoss: 2.096433\n",
      "Train Epoch: 4 [960/1280 (75%)]\tLoss: 2.057897\n",
      "Train Epoch: 4 [1152/1280 (90%)]\tLoss: 2.022720\n",
      "\n",
      "Test set: Average loss: 2.0391, Accuracy: 5303/10048 (53%)\n",
      "\n",
      "Train Epoch: 5 [0/1280 (0%)]\tLoss: 2.031389\n",
      "Train Epoch: 5 [192/1280 (15%)]\tLoss: 1.995332\n",
      "Train Epoch: 5 [384/1280 (30%)]\tLoss: 1.986518\n",
      "Train Epoch: 5 [576/1280 (45%)]\tLoss: 1.993174\n",
      "Train Epoch: 5 [768/1280 (60%)]\tLoss: 1.936458\n",
      "Train Epoch: 5 [960/1280 (75%)]\tLoss: 1.868216\n",
      "Train Epoch: 5 [1152/1280 (90%)]\tLoss: 1.813440\n",
      "\n",
      "Test set: Average loss: 1.8388, Accuracy: 5889/10048 (59%)\n",
      "\n",
      "Train Epoch: 6 [0/1280 (0%)]\tLoss: 1.824537\n",
      "Train Epoch: 6 [192/1280 (15%)]\tLoss: 1.736828\n",
      "Train Epoch: 6 [384/1280 (30%)]\tLoss: 1.742806\n",
      "Train Epoch: 6 [576/1280 (45%)]\tLoss: 1.753664\n",
      "Train Epoch: 6 [768/1280 (60%)]\tLoss: 1.646983\n",
      "Train Epoch: 6 [960/1280 (75%)]\tLoss: 1.547400\n",
      "Train Epoch: 6 [1152/1280 (90%)]\tLoss: 1.471260\n",
      "\n",
      "Test set: Average loss: 1.5170, Accuracy: 6542/10048 (65%)\n",
      "\n",
      "Train Epoch: 7 [0/1280 (0%)]\tLoss: 1.494760\n",
      "Train Epoch: 7 [192/1280 (15%)]\tLoss: 1.349620\n",
      "Train Epoch: 7 [384/1280 (30%)]\tLoss: 1.383262\n",
      "Train Epoch: 7 [576/1280 (45%)]\tLoss: 1.418486\n",
      "Train Epoch: 7 [768/1280 (60%)]\tLoss: 1.245118\n",
      "Train Epoch: 7 [960/1280 (75%)]\tLoss: 1.150551\n",
      "Train Epoch: 7 [1152/1280 (90%)]\tLoss: 1.066777\n",
      "\n",
      "Test set: Average loss: 1.1494, Accuracy: 7278/10048 (72%)\n",
      "\n",
      "Train Epoch: 8 [0/1280 (0%)]\tLoss: 1.122929\n",
      "Train Epoch: 8 [192/1280 (15%)]\tLoss: 0.956471\n",
      "Train Epoch: 8 [384/1280 (30%)]\tLoss: 1.019274\n",
      "Train Epoch: 8 [576/1280 (45%)]\tLoss: 1.095761\n",
      "Train Epoch: 8 [768/1280 (60%)]\tLoss: 0.885860\n",
      "Train Epoch: 8 [960/1280 (75%)]\tLoss: 0.836435\n",
      "Train Epoch: 8 [1152/1280 (90%)]\tLoss: 0.744190\n",
      "\n",
      "Test set: Average loss: 0.8674, Accuracy: 7717/10048 (77%)\n",
      "\n",
      "Train Epoch: 9 [0/1280 (0%)]\tLoss: 0.848259\n",
      "Train Epoch: 9 [192/1280 (15%)]\tLoss: 0.685104\n",
      "Train Epoch: 9 [384/1280 (30%)]\tLoss: 0.762769\n",
      "Train Epoch: 9 [576/1280 (45%)]\tLoss: 0.861399\n",
      "Train Epoch: 9 [768/1280 (60%)]\tLoss: 0.659047\n",
      "Train Epoch: 9 [960/1280 (75%)]\tLoss: 0.658028\n",
      "Train Epoch: 9 [1152/1280 (90%)]\tLoss: 0.540727\n",
      "\n",
      "Test set: Average loss: 0.6948, Accuracy: 8047/10048 (80%)\n",
      "\n",
      "Train Epoch: 10 [0/1280 (0%)]\tLoss: 0.679232\n",
      "Train Epoch: 10 [192/1280 (15%)]\tLoss: 0.535569\n",
      "Train Epoch: 10 [384/1280 (30%)]\tLoss: 0.609580\n",
      "Train Epoch: 10 [576/1280 (45%)]\tLoss: 0.707388\n",
      "Train Epoch: 10 [768/1280 (60%)]\tLoss: 0.534663\n",
      "Train Epoch: 10 [960/1280 (75%)]\tLoss: 0.562358\n",
      "Train Epoch: 10 [1152/1280 (90%)]\tLoss: 0.417704\n",
      "\n",
      "Test set: Average loss: 0.5930, Accuracy: 8232/10048 (82%)\n",
      "\n",
      "Train Epoch: 11 [0/1280 (0%)]\tLoss: 0.575928\n",
      "Train Epoch: 11 [192/1280 (15%)]\tLoss: 0.460046\n",
      "Train Epoch: 11 [384/1280 (30%)]\tLoss: 0.513889\n",
      "Train Epoch: 11 [576/1280 (45%)]\tLoss: 0.600436\n",
      "Train Epoch: 11 [768/1280 (60%)]\tLoss: 0.459983\n",
      "Train Epoch: 11 [960/1280 (75%)]\tLoss: 0.503224\n",
      "Train Epoch: 11 [1152/1280 (90%)]\tLoss: 0.340457\n",
      "\n",
      "Test set: Average loss: 0.5287, Accuracy: 8368/10048 (83%)\n",
      "\n",
      "Train Epoch: 12 [0/1280 (0%)]\tLoss: 0.511515\n",
      "Train Epoch: 12 [192/1280 (15%)]\tLoss: 0.418133\n",
      "Train Epoch: 12 [384/1280 (30%)]\tLoss: 0.449493\n",
      "Train Epoch: 12 [576/1280 (45%)]\tLoss: 0.521820\n",
      "Train Epoch: 12 [768/1280 (60%)]\tLoss: 0.413445\n",
      "Train Epoch: 12 [960/1280 (75%)]\tLoss: 0.459713\n",
      "Train Epoch: 12 [1152/1280 (90%)]\tLoss: 0.288398\n",
      "\n",
      "Test set: Average loss: 0.4810, Accuracy: 8485/10048 (84%)\n",
      "\n",
      "Train Epoch: 13 [0/1280 (0%)]\tLoss: 0.466479\n",
      "Train Epoch: 13 [192/1280 (15%)]\tLoss: 0.390574\n",
      "Train Epoch: 13 [384/1280 (30%)]\tLoss: 0.404029\n",
      "Train Epoch: 13 [576/1280 (45%)]\tLoss: 0.460155\n",
      "Train Epoch: 13 [768/1280 (60%)]\tLoss: 0.384089\n",
      "Train Epoch: 13 [960/1280 (75%)]\tLoss: 0.421874\n",
      "Train Epoch: 13 [1152/1280 (90%)]\tLoss: 0.250754\n",
      "\n",
      "Test set: Average loss: 0.4430, Accuracy: 8613/10048 (86%)\n",
      "\n",
      "Train Epoch: 14 [0/1280 (0%)]\tLoss: 0.433127\n",
      "Train Epoch: 14 [192/1280 (15%)]\tLoss: 0.369122\n",
      "Train Epoch: 14 [384/1280 (30%)]\tLoss: 0.369399\n",
      "Train Epoch: 14 [576/1280 (45%)]\tLoss: 0.411417\n",
      "Train Epoch: 14 [768/1280 (60%)]\tLoss: 0.363703\n",
      "Train Epoch: 14 [960/1280 (75%)]\tLoss: 0.387599\n",
      "Train Epoch: 14 [1152/1280 (90%)]\tLoss: 0.222247\n",
      "\n",
      "Test set: Average loss: 0.4119, Accuracy: 8723/10048 (87%)\n",
      "\n",
      "Train Epoch: 15 [0/1280 (0%)]\tLoss: 0.406657\n",
      "Train Epoch: 15 [192/1280 (15%)]\tLoss: 0.350674\n",
      "Train Epoch: 15 [384/1280 (30%)]\tLoss: 0.342641\n",
      "Train Epoch: 15 [576/1280 (45%)]\tLoss: 0.371906\n",
      "Train Epoch: 15 [768/1280 (60%)]\tLoss: 0.346526\n",
      "Train Epoch: 15 [960/1280 (75%)]\tLoss: 0.357644\n",
      "Train Epoch: 15 [1152/1280 (90%)]\tLoss: 0.199996\n",
      "\n",
      "Test set: Average loss: 0.3870, Accuracy: 8797/10048 (88%)\n",
      "\n",
      "Train Epoch: 16 [0/1280 (0%)]\tLoss: 0.385585\n",
      "Train Epoch: 16 [192/1280 (15%)]\tLoss: 0.334336\n",
      "Train Epoch: 16 [384/1280 (30%)]\tLoss: 0.320408\n",
      "Train Epoch: 16 [576/1280 (45%)]\tLoss: 0.339654\n",
      "Train Epoch: 16 [768/1280 (60%)]\tLoss: 0.329906\n",
      "Train Epoch: 16 [960/1280 (75%)]\tLoss: 0.332142\n",
      "Train Epoch: 16 [1152/1280 (90%)]\tLoss: 0.181941\n",
      "\n",
      "Test set: Average loss: 0.3666, Accuracy: 8870/10048 (88%)\n",
      "\n",
      "Train Epoch: 17 [0/1280 (0%)]\tLoss: 0.367308\n",
      "Train Epoch: 17 [192/1280 (15%)]\tLoss: 0.320162\n",
      "Train Epoch: 17 [384/1280 (30%)]\tLoss: 0.302333\n",
      "Train Epoch: 17 [576/1280 (45%)]\tLoss: 0.312760\n",
      "Train Epoch: 17 [768/1280 (60%)]\tLoss: 0.313044\n",
      "Train Epoch: 17 [960/1280 (75%)]\tLoss: 0.309768\n",
      "Train Epoch: 17 [1152/1280 (90%)]\tLoss: 0.166801\n",
      "\n",
      "Test set: Average loss: 0.3493, Accuracy: 8933/10048 (89%)\n",
      "\n",
      "Train Epoch: 18 [0/1280 (0%)]\tLoss: 0.350287\n",
      "Train Epoch: 18 [192/1280 (15%)]\tLoss: 0.306865\n",
      "Train Epoch: 18 [384/1280 (30%)]\tLoss: 0.286086\n",
      "Train Epoch: 18 [576/1280 (45%)]\tLoss: 0.290011\n",
      "Train Epoch: 18 [768/1280 (60%)]\tLoss: 0.296520\n",
      "Train Epoch: 18 [960/1280 (75%)]\tLoss: 0.290910\n",
      "Train Epoch: 18 [1152/1280 (90%)]\tLoss: 0.153903\n",
      "\n",
      "Test set: Average loss: 0.3350, Accuracy: 8980/10048 (89%)\n",
      "\n",
      "Train Epoch: 19 [0/1280 (0%)]\tLoss: 0.335870\n",
      "Train Epoch: 19 [192/1280 (15%)]\tLoss: 0.295220\n",
      "Train Epoch: 19 [384/1280 (30%)]\tLoss: 0.271954\n",
      "Train Epoch: 19 [576/1280 (45%)]\tLoss: 0.270630\n",
      "Train Epoch: 19 [768/1280 (60%)]\tLoss: 0.281089\n",
      "Train Epoch: 19 [960/1280 (75%)]\tLoss: 0.275093\n",
      "Train Epoch: 19 [1152/1280 (90%)]\tLoss: 0.142614\n",
      "\n",
      "Test set: Average loss: 0.3231, Accuracy: 9017/10048 (90%)\n",
      "\n",
      "Train Epoch: 20 [0/1280 (0%)]\tLoss: 0.322655\n",
      "Train Epoch: 20 [192/1280 (15%)]\tLoss: 0.283878\n",
      "Train Epoch: 20 [384/1280 (30%)]\tLoss: 0.259359\n",
      "Train Epoch: 20 [576/1280 (45%)]\tLoss: 0.253568\n",
      "Train Epoch: 20 [768/1280 (60%)]\tLoss: 0.266387\n",
      "Train Epoch: 20 [960/1280 (75%)]\tLoss: 0.261486\n",
      "Train Epoch: 20 [1152/1280 (90%)]\tLoss: 0.132795\n",
      "\n",
      "Test set: Average loss: 0.3126, Accuracy: 9050/10048 (90%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "model = Net().to(device)\n",
    "optimizer =  optim.SGD(model.parameters(), lr=args.lr)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, device, model, train_dataloader, optimizer, epoch)\n",
    "    test(args, device, model, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
