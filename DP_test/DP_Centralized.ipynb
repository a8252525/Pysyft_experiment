{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use centrailzed training to compare with federated learning\n",
    "epochs = 30\n",
    "n_train_items = 12800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Arguments():\n",
    "    def __init__(self):\n",
    "        self.batch_size = 64\n",
    "        self.test_batch_size = 64\n",
    "        self.epochs = epochs\n",
    "        self.lr = 0.01\n",
    "        self.momentum = 0.5\n",
    "        self.no_cuda = False\n",
    "        self.seed = 1\n",
    "        self.log_interval = 30\n",
    "        self.save_model = False\n",
    "\n",
    "args = Arguments()\n",
    "\n",
    "use_cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "\n",
    "torch.manual_seed(args.seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=True, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.batch_size\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('../../data', train=False, download=True, transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=args.test_batch_size\n",
    ")\n",
    "\n",
    "    \n",
    "#---\n",
    "\n",
    "train_dataloader = [\n",
    "        ((data), (target))\n",
    "        for i, (data, target) in enumerate(train_loader)\n",
    "        if i < n_train_items / args.batch_size\n",
    "    ]\n",
    "# test_dataloader = [\n",
    "#         ((data), (target))\n",
    "#         for i, (data, target) in enumerate(test_loader)\n",
    "#         if i < n_train_items / args.batch_size\n",
    "#     ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n",
    "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
    "        self.fc1 = nn.Linear(4*4*50, 500)\n",
    "        self.fc2 = nn.Linear(500, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2, 2)\n",
    "        x = x.view(-1, 4*4*50)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args, device, model, train_dataloader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_dataloader):\n",
    "        data,target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm(model.parameters(),3)\n",
    "        optimizer.step()\n",
    "        \n",
    "        \n",
    "        \n",
    "        if batch_idx % args.log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * args.batch_size, len(train_dataloader) * args.batch_size,\n",
    "                100. * batch_idx / len(train_dataloader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(args, device, model, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        \n",
    "        # add gussian noise before testing\n",
    "        for param in model.parameters():\n",
    "            param.add_(torch.from_numpy(np.random.normal(0,0.05,param.size())).to(device))\n",
    "        \n",
    "        \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item() # sum up batch loss\n",
    "            pred = output.argmax(1, keepdim=True) # get the index of the max log-probability \n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "    test_loss /= len(test_loader)*(args.batch_size)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader)* (args.batch_size),\n",
    "        100. * correct / (len(test_loader)*args.batch_size)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 1, 5, 5])\n",
      "torch.Size([20])\n",
      "torch.Size([50, 20, 5, 5])\n",
      "torch.Size([50])\n",
      "torch.Size([500, 800])\n",
      "torch.Size([500])\n",
      "torch.Size([10, 500])\n",
      "torch.Size([10])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for param in model.parameters():\n",
    "        print(param.size())\n",
    "        param.add_(torch.from_numpy(np.random.normal(0,0.5,param.size())).to(device))\n",
    "# print(list(model.parameters())[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:9: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n",
      "  if __name__ == '__main__':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/12800 (0%)]\tLoss: 2.299716\n",
      "Train Epoch: 1 [1920/12800 (15%)]\tLoss: 2.154342\n",
      "Train Epoch: 1 [3840/12800 (30%)]\tLoss: 1.877676\n",
      "Train Epoch: 1 [5760/12800 (45%)]\tLoss: 1.357273\n",
      "Train Epoch: 1 [7680/12800 (60%)]\tLoss: 0.905127\n",
      "Train Epoch: 1 [9600/12800 (75%)]\tLoss: 0.496682\n",
      "Train Epoch: 1 [11520/12800 (90%)]\tLoss: 0.718168\n",
      "\n",
      "Test set: Average loss: 2.2897, Accuracy: 3838/10048 (38%)\n",
      "\n",
      "Train Epoch: 2 [0/12800 (0%)]\tLoss: 2.308139\n",
      "Train Epoch: 2 [1920/12800 (15%)]\tLoss: 0.564057\n",
      "Train Epoch: 2 [3840/12800 (30%)]\tLoss: 0.368379\n",
      "Train Epoch: 2 [5760/12800 (45%)]\tLoss: 0.315816\n",
      "Train Epoch: 2 [7680/12800 (60%)]\tLoss: 0.414582\n",
      "Train Epoch: 2 [9600/12800 (75%)]\tLoss: 0.359035\n",
      "Train Epoch: 2 [11520/12800 (90%)]\tLoss: 0.575282\n",
      "\n",
      "Test set: Average loss: 10.6912, Accuracy: 1967/10048 (20%)\n",
      "\n",
      "Train Epoch: 3 [0/12800 (0%)]\tLoss: 10.663928\n",
      "Train Epoch: 3 [1920/12800 (15%)]\tLoss: 0.857765\n",
      "Train Epoch: 3 [3840/12800 (30%)]\tLoss: 0.547203\n",
      "Train Epoch: 3 [5760/12800 (45%)]\tLoss: 0.324117\n",
      "Train Epoch: 3 [7680/12800 (60%)]\tLoss: 0.647007\n",
      "Train Epoch: 3 [9600/12800 (75%)]\tLoss: 0.237739\n",
      "Train Epoch: 3 [11520/12800 (90%)]\tLoss: 0.905089\n",
      "\n",
      "Test set: Average loss: 6.8720, Accuracy: 3081/10048 (31%)\n",
      "\n",
      "Train Epoch: 4 [0/12800 (0%)]\tLoss: 7.212550\n",
      "Train Epoch: 4 [1920/12800 (15%)]\tLoss: 1.185103\n",
      "Train Epoch: 4 [3840/12800 (30%)]\tLoss: 0.518532\n",
      "Train Epoch: 4 [5760/12800 (45%)]\tLoss: 0.443560\n",
      "Train Epoch: 4 [7680/12800 (60%)]\tLoss: 0.740784\n",
      "Train Epoch: 4 [9600/12800 (75%)]\tLoss: 0.415929\n",
      "Train Epoch: 4 [11520/12800 (90%)]\tLoss: 1.024515\n",
      "\n",
      "Test set: Average loss: 13.3917, Accuracy: 2884/10048 (29%)\n",
      "\n",
      "Train Epoch: 5 [0/12800 (0%)]\tLoss: 14.592633\n",
      "Train Epoch: 5 [1920/12800 (15%)]\tLoss: 1.463346\n",
      "Train Epoch: 5 [3840/12800 (30%)]\tLoss: 0.476446\n",
      "Train Epoch: 5 [5760/12800 (45%)]\tLoss: 1.212397\n",
      "Train Epoch: 5 [7680/12800 (60%)]\tLoss: 1.140746\n",
      "Train Epoch: 5 [9600/12800 (75%)]\tLoss: 0.504450\n",
      "Train Epoch: 5 [11520/12800 (90%)]\tLoss: 1.029424\n",
      "\n",
      "Test set: Average loss: 20.8227, Accuracy: 1782/10048 (18%)\n",
      "\n",
      "Train Epoch: 6 [0/12800 (0%)]\tLoss: 23.050814\n",
      "Train Epoch: 6 [1920/12800 (15%)]\tLoss: 1.883806\n",
      "Train Epoch: 6 [3840/12800 (30%)]\tLoss: 1.567532\n",
      "Train Epoch: 6 [5760/12800 (45%)]\tLoss: 0.885480\n",
      "Train Epoch: 6 [7680/12800 (60%)]\tLoss: 0.951286\n",
      "Train Epoch: 6 [9600/12800 (75%)]\tLoss: 0.749036\n",
      "Train Epoch: 6 [11520/12800 (90%)]\tLoss: 0.939026\n",
      "\n",
      "Test set: Average loss: 14.9814, Accuracy: 3571/10048 (36%)\n",
      "\n",
      "Train Epoch: 7 [0/12800 (0%)]\tLoss: 13.974225\n",
      "Train Epoch: 7 [1920/12800 (15%)]\tLoss: 1.531683\n",
      "Train Epoch: 7 [3840/12800 (30%)]\tLoss: 1.149157\n",
      "Train Epoch: 7 [5760/12800 (45%)]\tLoss: 1.332292\n",
      "Train Epoch: 7 [7680/12800 (60%)]\tLoss: 0.736768\n",
      "Train Epoch: 7 [9600/12800 (75%)]\tLoss: 1.115296\n",
      "Train Epoch: 7 [11520/12800 (90%)]\tLoss: 1.625732\n",
      "\n",
      "Test set: Average loss: 9.1672, Accuracy: 4936/10048 (49%)\n",
      "\n",
      "Train Epoch: 8 [0/12800 (0%)]\tLoss: 7.053475\n",
      "Train Epoch: 8 [1920/12800 (15%)]\tLoss: 1.873940\n",
      "Train Epoch: 8 [3840/12800 (30%)]\tLoss: 0.779607\n",
      "Train Epoch: 8 [5760/12800 (45%)]\tLoss: 1.244203\n",
      "Train Epoch: 8 [7680/12800 (60%)]\tLoss: 0.651639\n",
      "Train Epoch: 8 [9600/12800 (75%)]\tLoss: 1.016967\n",
      "Train Epoch: 8 [11520/12800 (90%)]\tLoss: 1.653082\n",
      "\n",
      "Test set: Average loss: 11.3226, Accuracy: 4203/10048 (42%)\n",
      "\n",
      "Train Epoch: 9 [0/12800 (0%)]\tLoss: 9.571106\n",
      "Train Epoch: 9 [1920/12800 (15%)]\tLoss: 2.711775\n",
      "Train Epoch: 9 [3840/12800 (30%)]\tLoss: 0.537890\n",
      "Train Epoch: 9 [5760/12800 (45%)]\tLoss: 1.874412\n",
      "Train Epoch: 9 [7680/12800 (60%)]\tLoss: 1.035951\n",
      "Train Epoch: 9 [9600/12800 (75%)]\tLoss: 1.389245\n",
      "Train Epoch: 9 [11520/12800 (90%)]\tLoss: 1.689363\n",
      "\n",
      "Test set: Average loss: 6.4752, Accuracy: 6092/10048 (61%)\n",
      "\n",
      "Train Epoch: 10 [0/12800 (0%)]\tLoss: 4.065559\n",
      "Train Epoch: 10 [1920/12800 (15%)]\tLoss: 1.318418\n",
      "Train Epoch: 10 [3840/12800 (30%)]\tLoss: 1.371112\n",
      "Train Epoch: 10 [5760/12800 (45%)]\tLoss: 1.028995\n",
      "Train Epoch: 10 [7680/12800 (60%)]\tLoss: 0.756345\n",
      "Train Epoch: 10 [9600/12800 (75%)]\tLoss: 1.277309\n",
      "Train Epoch: 10 [11520/12800 (90%)]\tLoss: 3.004989\n",
      "\n",
      "Test set: Average loss: 15.7224, Accuracy: 3883/10048 (39%)\n",
      "\n",
      "Train Epoch: 11 [0/12800 (0%)]\tLoss: 16.682892\n",
      "Train Epoch: 11 [1920/12800 (15%)]\tLoss: 1.670028\n",
      "Train Epoch: 11 [3840/12800 (30%)]\tLoss: 0.667778\n",
      "Train Epoch: 11 [5760/12800 (45%)]\tLoss: 1.661564\n",
      "Train Epoch: 11 [7680/12800 (60%)]\tLoss: 1.093868\n",
      "Train Epoch: 11 [9600/12800 (75%)]\tLoss: 1.523087\n",
      "Train Epoch: 11 [11520/12800 (90%)]\tLoss: 2.530493\n",
      "\n",
      "Test set: Average loss: 10.5375, Accuracy: 5322/10048 (53%)\n",
      "\n",
      "Train Epoch: 12 [0/12800 (0%)]\tLoss: 11.641361\n",
      "Train Epoch: 12 [1920/12800 (15%)]\tLoss: 1.209502\n",
      "Train Epoch: 12 [3840/12800 (30%)]\tLoss: 0.529584\n",
      "Train Epoch: 12 [5760/12800 (45%)]\tLoss: 1.690329\n",
      "Train Epoch: 12 [7680/12800 (60%)]\tLoss: 1.589030\n",
      "Train Epoch: 12 [9600/12800 (75%)]\tLoss: 0.882651\n",
      "Train Epoch: 12 [11520/12800 (90%)]\tLoss: 3.091187\n",
      "\n",
      "Test set: Average loss: 11.5075, Accuracy: 5402/10048 (54%)\n",
      "\n",
      "Train Epoch: 13 [0/12800 (0%)]\tLoss: 11.851180\n",
      "Train Epoch: 13 [1920/12800 (15%)]\tLoss: 1.902340\n",
      "Train Epoch: 13 [3840/12800 (30%)]\tLoss: 1.318549\n",
      "Train Epoch: 13 [5760/12800 (45%)]\tLoss: 0.933416\n",
      "Train Epoch: 13 [7680/12800 (60%)]\tLoss: 0.570196\n",
      "Train Epoch: 13 [9600/12800 (75%)]\tLoss: 0.563135\n",
      "Train Epoch: 13 [11520/12800 (90%)]\tLoss: 2.125890\n",
      "\n",
      "Test set: Average loss: 12.7210, Accuracy: 5427/10048 (54%)\n",
      "\n",
      "Train Epoch: 14 [0/12800 (0%)]\tLoss: 13.411083\n",
      "Train Epoch: 14 [1920/12800 (15%)]\tLoss: 2.062871\n",
      "Train Epoch: 14 [3840/12800 (30%)]\tLoss: 1.157411\n",
      "Train Epoch: 14 [5760/12800 (45%)]\tLoss: 1.274663\n",
      "Train Epoch: 14 [7680/12800 (60%)]\tLoss: 0.467153\n",
      "Train Epoch: 14 [9600/12800 (75%)]\tLoss: 0.677934\n",
      "Train Epoch: 14 [11520/12800 (90%)]\tLoss: 2.819052\n",
      "\n",
      "Test set: Average loss: 15.2681, Accuracy: 5225/10048 (52%)\n",
      "\n",
      "Train Epoch: 15 [0/12800 (0%)]\tLoss: 17.711113\n",
      "Train Epoch: 15 [1920/12800 (15%)]\tLoss: 1.794983\n",
      "Train Epoch: 15 [3840/12800 (30%)]\tLoss: 0.685390\n",
      "Train Epoch: 15 [5760/12800 (45%)]\tLoss: 1.153953\n",
      "Train Epoch: 15 [7680/12800 (60%)]\tLoss: 1.902148\n",
      "Train Epoch: 15 [9600/12800 (75%)]\tLoss: 1.035561\n",
      "Train Epoch: 15 [11520/12800 (90%)]\tLoss: 3.848547\n",
      "\n",
      "Test set: Average loss: 7.3458, Accuracy: 6825/10048 (68%)\n",
      "\n",
      "Train Epoch: 16 [0/12800 (0%)]\tLoss: 6.787663\n",
      "Train Epoch: 16 [1920/12800 (15%)]\tLoss: 1.405427\n",
      "Train Epoch: 16 [3840/12800 (30%)]\tLoss: 0.372111\n",
      "Train Epoch: 16 [5760/12800 (45%)]\tLoss: 2.296216\n",
      "Train Epoch: 16 [7680/12800 (60%)]\tLoss: 1.478778\n",
      "Train Epoch: 16 [9600/12800 (75%)]\tLoss: 1.016029\n",
      "Train Epoch: 16 [11520/12800 (90%)]\tLoss: 3.572329\n",
      "\n",
      "Test set: Average loss: 17.1945, Accuracy: 5395/10048 (54%)\n",
      "\n",
      "Train Epoch: 17 [0/12800 (0%)]\tLoss: 17.699171\n",
      "Train Epoch: 17 [1920/12800 (15%)]\tLoss: 1.658798\n",
      "Train Epoch: 17 [3840/12800 (30%)]\tLoss: 0.300782\n",
      "Train Epoch: 17 [5760/12800 (45%)]\tLoss: 2.644514\n",
      "Train Epoch: 17 [7680/12800 (60%)]\tLoss: 1.203020\n",
      "Train Epoch: 17 [9600/12800 (75%)]\tLoss: 1.171363\n",
      "Train Epoch: 17 [11520/12800 (90%)]\tLoss: 3.193809\n",
      "\n",
      "Test set: Average loss: 11.1516, Accuracy: 6697/10048 (67%)\n",
      "\n",
      "Train Epoch: 18 [0/12800 (0%)]\tLoss: 9.841335\n",
      "Train Epoch: 18 [1920/12800 (15%)]\tLoss: 1.353781\n",
      "Train Epoch: 18 [3840/12800 (30%)]\tLoss: 1.209353\n",
      "Train Epoch: 18 [5760/12800 (45%)]\tLoss: 3.013154\n",
      "Train Epoch: 18 [7680/12800 (60%)]\tLoss: 0.983723\n",
      "Train Epoch: 18 [9600/12800 (75%)]\tLoss: 1.560121\n",
      "Train Epoch: 18 [11520/12800 (90%)]\tLoss: 3.607391\n",
      "\n",
      "Test set: Average loss: 9.9926, Accuracy: 6319/10048 (63%)\n",
      "\n",
      "Train Epoch: 19 [0/12800 (0%)]\tLoss: 12.095329\n",
      "Train Epoch: 19 [1920/12800 (15%)]\tLoss: 2.796739\n",
      "Train Epoch: 19 [3840/12800 (30%)]\tLoss: 3.758548\n",
      "Train Epoch: 19 [5760/12800 (45%)]\tLoss: 2.711555\n",
      "Train Epoch: 19 [7680/12800 (60%)]\tLoss: 2.262206\n",
      "Train Epoch: 19 [9600/12800 (75%)]\tLoss: 1.305805\n",
      "Train Epoch: 19 [11520/12800 (90%)]\tLoss: 4.020818\n",
      "\n",
      "Test set: Average loss: 20.0801, Accuracy: 5263/10048 (52%)\n",
      "\n",
      "Train Epoch: 20 [0/12800 (0%)]\tLoss: 24.014141\n",
      "Train Epoch: 20 [1920/12800 (15%)]\tLoss: 1.251084\n",
      "Train Epoch: 20 [3840/12800 (30%)]\tLoss: 2.307009\n",
      "Train Epoch: 20 [5760/12800 (45%)]\tLoss: 2.848084\n",
      "Train Epoch: 20 [7680/12800 (60%)]\tLoss: 3.478482\n",
      "Train Epoch: 20 [9600/12800 (75%)]\tLoss: 2.143148\n",
      "Train Epoch: 20 [11520/12800 (90%)]\tLoss: 3.055179\n",
      "\n",
      "Test set: Average loss: 16.4040, Accuracy: 6427/10048 (64%)\n",
      "\n",
      "Train Epoch: 21 [0/12800 (0%)]\tLoss: 12.573646\n",
      "Train Epoch: 21 [1920/12800 (15%)]\tLoss: 2.411456\n",
      "Train Epoch: 21 [3840/12800 (30%)]\tLoss: 2.818104\n",
      "Train Epoch: 21 [5760/12800 (45%)]\tLoss: 4.317619\n",
      "Train Epoch: 21 [7680/12800 (60%)]\tLoss: 3.827865\n",
      "Train Epoch: 21 [9600/12800 (75%)]\tLoss: 1.366085\n",
      "Train Epoch: 21 [11520/12800 (90%)]\tLoss: 5.920724\n",
      "\n",
      "Test set: Average loss: 18.8407, Accuracy: 5636/10048 (56%)\n",
      "\n",
      "Train Epoch: 22 [0/12800 (0%)]\tLoss: 27.793602\n",
      "Train Epoch: 22 [1920/12800 (15%)]\tLoss: 2.278187\n",
      "Train Epoch: 22 [3840/12800 (30%)]\tLoss: 2.372218\n",
      "Train Epoch: 22 [5760/12800 (45%)]\tLoss: 6.300216\n",
      "Train Epoch: 22 [7680/12800 (60%)]\tLoss: 1.490223\n",
      "Train Epoch: 22 [9600/12800 (75%)]\tLoss: 1.375864\n",
      "Train Epoch: 22 [11520/12800 (90%)]\tLoss: 3.479467\n",
      "\n",
      "Test set: Average loss: 10.4826, Accuracy: 6855/10048 (68%)\n",
      "\n",
      "Train Epoch: 23 [0/12800 (0%)]\tLoss: 11.463903\n",
      "Train Epoch: 23 [1920/12800 (15%)]\tLoss: 4.081833\n",
      "Train Epoch: 23 [3840/12800 (30%)]\tLoss: 2.994797\n",
      "Train Epoch: 23 [5760/12800 (45%)]\tLoss: 4.027921\n",
      "Train Epoch: 23 [7680/12800 (60%)]\tLoss: 2.565597\n",
      "Train Epoch: 23 [9600/12800 (75%)]\tLoss: 2.303364\n",
      "Train Epoch: 23 [11520/12800 (90%)]\tLoss: 2.877402\n",
      "\n",
      "Test set: Average loss: 14.3079, Accuracy: 6349/10048 (63%)\n",
      "\n",
      "Train Epoch: 24 [0/12800 (0%)]\tLoss: 11.722322\n",
      "Train Epoch: 24 [1920/12800 (15%)]\tLoss: 4.119606\n",
      "Train Epoch: 24 [3840/12800 (30%)]\tLoss: 3.273461\n",
      "Train Epoch: 24 [5760/12800 (45%)]\tLoss: 6.135668\n",
      "Train Epoch: 24 [7680/12800 (60%)]\tLoss: 3.519075\n",
      "Train Epoch: 24 [9600/12800 (75%)]\tLoss: 2.133728\n",
      "Train Epoch: 24 [11520/12800 (90%)]\tLoss: 2.757596\n",
      "\n",
      "Test set: Average loss: 21.6971, Accuracy: 6021/10048 (60%)\n",
      "\n",
      "Train Epoch: 25 [0/12800 (0%)]\tLoss: 23.699507\n",
      "Train Epoch: 25 [1920/12800 (15%)]\tLoss: 4.160842\n",
      "Train Epoch: 25 [3840/12800 (30%)]\tLoss: 2.558153\n",
      "Train Epoch: 25 [5760/12800 (45%)]\tLoss: 6.512671\n",
      "Train Epoch: 25 [7680/12800 (60%)]\tLoss: 2.559749\n",
      "Train Epoch: 25 [9600/12800 (75%)]\tLoss: 0.827999\n",
      "Train Epoch: 25 [11520/12800 (90%)]\tLoss: 2.630529\n",
      "\n",
      "Test set: Average loss: 9.4141, Accuracy: 7182/10048 (71%)\n",
      "\n",
      "Train Epoch: 26 [0/12800 (0%)]\tLoss: 10.843781\n",
      "Train Epoch: 26 [1920/12800 (15%)]\tLoss: 5.537088\n",
      "Train Epoch: 26 [3840/12800 (30%)]\tLoss: 1.797728\n",
      "Train Epoch: 26 [5760/12800 (45%)]\tLoss: 5.737238\n",
      "Train Epoch: 26 [7680/12800 (60%)]\tLoss: 2.382852\n",
      "Train Epoch: 26 [9600/12800 (75%)]\tLoss: 1.035343\n",
      "Train Epoch: 26 [11520/12800 (90%)]\tLoss: 2.007204\n",
      "\n",
      "Test set: Average loss: 9.3741, Accuracy: 7533/10048 (75%)\n",
      "\n",
      "Train Epoch: 27 [0/12800 (0%)]\tLoss: 7.292779\n",
      "Train Epoch: 27 [1920/12800 (15%)]\tLoss: 7.771553\n",
      "Train Epoch: 27 [3840/12800 (30%)]\tLoss: 1.355039\n",
      "Train Epoch: 27 [5760/12800 (45%)]\tLoss: 4.225106\n",
      "Train Epoch: 27 [7680/12800 (60%)]\tLoss: 2.570191\n",
      "Train Epoch: 27 [9600/12800 (75%)]\tLoss: 1.023847\n",
      "Train Epoch: 27 [11520/12800 (90%)]\tLoss: 4.193612\n",
      "\n",
      "Test set: Average loss: 7.2925, Accuracy: 7998/10048 (80%)\n",
      "\n",
      "Train Epoch: 28 [0/12800 (0%)]\tLoss: 10.768291\n",
      "Train Epoch: 28 [1920/12800 (15%)]\tLoss: 2.623453\n",
      "Train Epoch: 28 [3840/12800 (30%)]\tLoss: 0.544671\n",
      "Train Epoch: 28 [5760/12800 (45%)]\tLoss: 4.060529\n",
      "Train Epoch: 28 [7680/12800 (60%)]\tLoss: 3.683452\n",
      "Train Epoch: 28 [9600/12800 (75%)]\tLoss: 1.607660\n",
      "Train Epoch: 28 [11520/12800 (90%)]\tLoss: 5.057990\n",
      "\n",
      "Test set: Average loss: 9.0669, Accuracy: 7636/10048 (76%)\n",
      "\n",
      "Train Epoch: 29 [0/12800 (0%)]\tLoss: 8.003888\n",
      "Train Epoch: 29 [1920/12800 (15%)]\tLoss: 1.380730\n",
      "Train Epoch: 29 [3840/12800 (30%)]\tLoss: 0.564841\n",
      "Train Epoch: 29 [5760/12800 (45%)]\tLoss: 3.796674\n",
      "Train Epoch: 29 [7680/12800 (60%)]\tLoss: 2.434839\n",
      "Train Epoch: 29 [9600/12800 (75%)]\tLoss: 1.741000\n",
      "Train Epoch: 29 [11520/12800 (90%)]\tLoss: 6.005457\n",
      "\n",
      "Test set: Average loss: 12.6898, Accuracy: 7298/10048 (73%)\n",
      "\n",
      "Train Epoch: 30 [0/12800 (0%)]\tLoss: 9.523649\n",
      "Train Epoch: 30 [1920/12800 (15%)]\tLoss: 3.712035\n",
      "Train Epoch: 30 [3840/12800 (30%)]\tLoss: 2.062226\n",
      "Train Epoch: 30 [5760/12800 (45%)]\tLoss: 3.887680\n",
      "Train Epoch: 30 [7680/12800 (60%)]\tLoss: 3.333468\n",
      "Train Epoch: 30 [9600/12800 (75%)]\tLoss: 1.230864\n",
      "Train Epoch: 30 [11520/12800 (90%)]\tLoss: 5.692647\n",
      "\n",
      "Test set: Average loss: 10.0476, Accuracy: 7730/10048 (77%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#%%time\n",
    "model = Net().to(device)\n",
    "optimizer =  optim.SGD(model.parameters(), lr=args.lr)\n",
    "for epoch in range(1, args.epochs + 1):\n",
    "    train(args, device, model, train_dataloader, optimizer, epoch)\n",
    "    test(args, device, model, test_loader)\n",
    "\n",
    "if (args.save_model):\n",
    "    torch.save(model.state_dict(), \"mnist_cnn.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
